{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Python & Math Prerequisites\n",
    "\n",
    "**Foundation for Deep Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master NumPy operations essential for deep learning\n",
    "- Understand key calculus concepts (derivatives, chain rule)\n",
    "- Know linear algebra fundamentals (vectors, matrices, operations)\n",
    "- Grasp probability basics needed for ML\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: NumPy Essentials for Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Why NumPy?\n",
    "\n",
    "Deep learning is fundamentally about **matrix operations**. NumPy provides:\n",
    "- Efficient array operations (vectorization)\n",
    "- Broadcasting for element-wise operations\n",
    "- Linear algebra routines\n",
    "\n",
    "PyTorch tensors are designed to feel like NumPy arrays, so mastering NumPy translates directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Array Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays from Python lists\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"1D array: {a}\")\n",
    "print(f\"Shape: {a.shape}, Dtype: {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array (matrix)\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "print(f\"2D array:\\n{b}\")\n",
    "print(f\"Shape: {b.shape}  # (rows, columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common initialization patterns\n",
    "zeros = np.zeros((3, 4))      # 3x4 matrix of zeros\n",
    "ones = np.ones((2, 3))        # 2x3 matrix of ones\n",
    "identity = np.eye(3)          # 3x3 identity matrix\n",
    "random_uniform = np.random.rand(2, 3)   # Uniform [0, 1)\n",
    "random_normal = np.random.randn(2, 3)   # Standard normal\n",
    "\n",
    "print(\"Zeros (3x4):\")\n",
    "print(zeros)\n",
    "print(\"\\nIdentity (3x3):\")\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Why Random Initialization Matters\n",
    "\n",
    "In neural networks, we initialize weights randomly (not zeros). This breaks symmetry - if all weights were the same, all neurons would learn the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Indexing and Slicing\n",
    "\n",
    "Understanding indexing is crucial for manipulating batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample matrix\n",
    "X = np.arange(12).reshape(3, 4)\n",
    "print(\"Matrix X:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "print(f\"Element at (0, 0): {X[0, 0]}\")\n",
    "print(f\"Element at (1, 2): {X[1, 2]}\")\n",
    "print(f\"First row: {X[0]}\")\n",
    "print(f\"First column: {X[:, 0]}\")\n",
    "print(f\"Last row: {X[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing: [start:end:step]\n",
    "print(f\"First two rows:\\n{X[:2]}\")\n",
    "print(f\"\\nColumns 1 to 3:\\n{X[:, 1:3]}\")\n",
    "print(f\"\\nEvery other column:\\n{X[:, ::2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "When working with batches:\n",
    "- `X[0]` - First sample in the batch\n",
    "- `X[:, 0]` - First feature across all samples\n",
    "- `X[:32]` - First mini-batch of 32 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reshaping Arrays\n",
    "\n",
    "Neural networks often require specific input shapes. Reshaping is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original array\n",
    "a = np.arange(12)\n",
    "print(f\"Original: {a}, shape: {a.shape}\")\n",
    "\n",
    "# Reshape to 2D\n",
    "b = a.reshape(3, 4)\n",
    "print(f\"\\nReshaped to (3, 4):\\n{b}\")\n",
    "\n",
    "# Using -1 for automatic dimension\n",
    "c = a.reshape(2, -1)  # -1 means \"figure it out\"\n",
    "print(f\"\\nReshaped to (2, -1) -> {c.shape}:\\n{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten - convert to 1D\n",
    "flattened = b.flatten()\n",
    "print(f\"Flattened: {flattened}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nOriginal (3, 4):\\n{b}\")\n",
    "print(f\"\\nTransposed (4, 3):\\n{b.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "Common reshaping patterns:\n",
    "- **Flattening images**: 28x28 image -> 784-element vector for dense layers\n",
    "- **Adding batch dimension**: (features,) -> (1, features)\n",
    "- **Transposing**: Switching between row vectors and column vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Broadcasting\n",
    "\n",
    "Broadcasting allows NumPy to perform operations on arrays of different shapes. This is **critical** for understanding how neural network operations work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar broadcasting\n",
    "a = np.array([1, 2, 3])\n",
    "print(f\"a = {a}\")\n",
    "print(f\"a + 10 = {a + 10}\")  # 10 is broadcast to [10, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector + Matrix broadcasting\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "bias = np.array([10, 20, 30])\n",
    "\n",
    "print(\"X (2x3):\")\n",
    "print(X)\n",
    "print(f\"\\nbias (3,): {bias}\")\n",
    "print(\"\\nX + bias (bias broadcast to each row):\")\n",
    "print(X + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules\n",
    "\n",
    "1. If arrays have different number of dimensions, pad the smaller one with 1s on the left\n",
    "2. Arrays are compatible if dimensions are equal OR one of them is 1\n",
    "3. The result shape is the maximum along each dimension\n",
    "\n",
    "**Example:** (2, 3) + (3,) -> (2, 3) + (1, 3) -> (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of broadcasting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# Matrix\n",
    "axes[0].imshow(X, cmap='Blues')\n",
    "axes[0].set_title('Matrix X (2x3)')\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axes[0].text(j, i, X[i, j], ha='center', va='center')\n",
    "\n",
    "# Bias (broadcast)\n",
    "bias_broadcast = np.tile(bias, (2, 1))\n",
    "axes[1].imshow(bias_broadcast, cmap='Oranges')\n",
    "axes[1].set_title('Bias (broadcast to 2x3)')\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axes[1].text(j, i, bias_broadcast[i, j], ha='center', va='center')\n",
    "\n",
    "# Result\n",
    "result = X + bias\n",
    "axes[2].imshow(result, cmap='Greens')\n",
    "axes[2].set_title('X + bias')\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axes[2].text(j, i, result[i, j], ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "Broadcasting is used in:\n",
    "- **Adding bias**: Output = W @ X + b (b is broadcast)\n",
    "- **Normalization**: (X - mean) / std\n",
    "- **Scaling**: X * scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Vectorization\n",
    "\n",
    "**Vectorization** means using array operations instead of loops. This is crucial for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large arrays\n",
    "n = 1000000\n",
    "a = np.random.randn(n)\n",
    "b = np.random.randn(n)\n",
    "\n",
    "# Loop approach (slow)\n",
    "start = time.time()\n",
    "c_loop = np.zeros(n)\n",
    "for i in range(n):\n",
    "    c_loop[i] = a[i] * b[i]\n",
    "loop_time = time.time() - start\n",
    "\n",
    "# Vectorized approach (fast)\n",
    "start = time.time()\n",
    "c_vec = a * b\n",
    "vec_time = time.time() - start\n",
    "\n",
    "print(f\"Loop time: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized time: {vec_time:.6f} seconds\")\n",
    "print(f\"Speedup: {loop_time/vec_time:.0f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of Thumb\n",
    "\n",
    "**Never use explicit Python loops for array operations.** Always look for a NumPy function or use vectorized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Linear Algebra Fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Vectors\n",
    "\n",
    "A **vector** is an ordered list of numbers. In deep learning:\n",
    "- Input features are vectors\n",
    "- Weights are vectors (or matrices)\n",
    "- Gradients are vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors in NumPy\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"v = {v}\")\n",
    "print(f\"w = {w}\")\n",
    "\n",
    "# Vector operations\n",
    "print(f\"\\nv + w = {v + w}\")      # Element-wise addition\n",
    "print(f\"v * w = {v * w}\")        # Element-wise multiplication\n",
    "print(f\"2 * v = {2 * v}\")        # Scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dot Product\n",
    "\n",
    "The **dot product** is the fundamental operation in neural networks. It computes a weighted sum.\n",
    "\n",
    "$$\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = u_1 v_1 + u_2 v_2 + ... + u_n v_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([4, 5, 6])\n",
    "\n",
    "# Manual calculation\n",
    "manual = 1*4 + 2*5 + 3*6\n",
    "print(f\"Manual: 1*4 + 2*5 + 3*6 = {manual}\")\n",
    "\n",
    "# NumPy\n",
    "print(f\"np.dot(v, w) = {np.dot(v, w)}\")\n",
    "print(f\"v @ w = {v @ w}\")  # @ operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context: The Neuron\n",
    "\n",
    "A single neuron computes:\n",
    "$$z = \\mathbf{w} \\cdot \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$$\n",
    "\n",
    "This is a dot product of weights and inputs, plus a bias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a single neuron\n",
    "inputs = np.array([0.5, 0.3, 0.2])     # 3 input features\n",
    "weights = np.array([0.4, 0.6, -0.2])   # 3 weights\n",
    "bias = 0.1\n",
    "\n",
    "# Weighted sum\n",
    "z = np.dot(weights, inputs) + bias\n",
    "print(f\"Inputs: {inputs}\")\n",
    "print(f\"Weights: {weights}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f\"Weighted sum z = w.x + b = {z:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Matrices\n",
    "\n",
    "A **matrix** is a 2D array of numbers. In neural networks, we use matrices to:\n",
    "- Represent weights between layers\n",
    "- Process batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matrices\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(f\"Matrix A (2x3):\")\n",
    "print(A)\n",
    "print(f\"\\nShape: {A.shape}\")\n",
    "print(f\"Number of rows: {A.shape[0]}\")\n",
    "print(f\"Number of columns: {A.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is the core operation in neural networks.\n",
    "\n",
    "For matrices A (m x n) and B (n x p), the result C = AB has shape (m x p).\n",
    "\n",
    "$$C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$\n",
    "\n",
    "**Key Rule:** Inner dimensions must match: (m x **n**) @ (**n** x p) = (m x p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])   # 3x2\n",
    "\n",
    "B = np.array([[7, 8, 9],\n",
    "              [10, 11, 12]])  # 2x3\n",
    "\n",
    "C = A @ B  # or np.matmul(A, B)\n",
    "\n",
    "print(f\"A (3x2):\\n{A}\")\n",
    "print(f\"\\nB (2x3):\\n{B}\")\n",
    "print(f\"\\nC = A @ B (3x3):\\n{C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify one element manually\n",
    "# C[0, 0] = A[0, :] . B[:, 0] = 1*7 + 2*10 = 27\n",
    "print(f\"C[0,0] = A[0,:] @ B[:,0] = {A[0,:]} @ {B[:,0]} = {A[0,:] @ B[:,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context: Layer as Matrix Multiplication\n",
    "\n",
    "A neural network layer transforms input X using weights W:\n",
    "$$\\mathbf{Z} = \\mathbf{X} \\mathbf{W}^T + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- X is (batch_size, input_features)\n",
    "- W is (output_features, input_features)\n",
    "- b is (output_features,)\n",
    "- Z is (batch_size, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a layer\n",
    "batch_size = 4\n",
    "input_features = 3\n",
    "output_features = 2\n",
    "\n",
    "# Input batch\n",
    "X = np.random.randn(batch_size, input_features)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "\n",
    "# Weights and bias\n",
    "W = np.random.randn(output_features, input_features)\n",
    "b = np.random.randn(output_features)\n",
    "print(f\"Weights W shape: {W.shape}\")\n",
    "print(f\"Bias b shape: {b.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "Z = X @ W.T + b  # b is broadcast!\n",
    "print(f\"\\nOutput Z shape: {Z.shape}\")\n",
    "print(f\"Output Z:\\n{Z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Vector Norms\n",
    "\n",
    "Norms measure the \"size\" or \"length\" of a vector.\n",
    "\n",
    "**L2 Norm (Euclidean):** $||\\mathbf{x}||_2 = \\sqrt{\\sum_i x_i^2}$\n",
    "\n",
    "**L1 Norm (Manhattan):** $||\\mathbf{x}||_1 = \\sum_i |x_i|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([3, 4])\n",
    "\n",
    "# L2 norm\n",
    "l2_manual = np.sqrt(3**2 + 4**2)\n",
    "l2_numpy = np.linalg.norm(x)\n",
    "print(f\"L2 norm: sqrt(3^2 + 4^2) = {l2_manual} = {l2_numpy}\")\n",
    "\n",
    "# L1 norm\n",
    "l1_manual = abs(3) + abs(4)\n",
    "l1_numpy = np.linalg.norm(x, ord=1)\n",
    "print(f\"L1 norm: |3| + |4| = {l1_manual} = {l1_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "Norms are used in:\n",
    "- **Regularization**: L1 (Lasso) and L2 (Ridge) penalties on weights\n",
    "- **Gradient clipping**: Limit gradient magnitude\n",
    "- **Normalization**: Dividing by norm to get unit vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Essential Calculus\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Derivatives\n",
    "\n",
    "A **derivative** measures how a function changes as its input changes. It's the slope of the tangent line.\n",
    "\n",
    "$$f'(x) = \\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical derivative approximation\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Example: f(x) = x^2, derivative = 2x\n",
    "f = lambda x: x**2\n",
    "\n",
    "x = 3\n",
    "numerical = numerical_derivative(f, x)\n",
    "analytical = 2 * x\n",
    "\n",
    "print(f\"f(x) = x^2 at x = {x}\")\n",
    "print(f\"Numerical derivative: {numerical:.6f}\")\n",
    "print(f\"Analytical derivative (2x): {analytical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize derivative as slope\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "y = x_range ** 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(x_range, y, 'b-', linewidth=2, label='f(x) = x^2')\n",
    "\n",
    "# Tangent line at x = 1\n",
    "x0 = 1\n",
    "slope = 2 * x0  # derivative at x0\n",
    "y0 = x0 ** 2\n",
    "tangent = slope * (x_range - x0) + y0\n",
    "\n",
    "ax.plot(x_range, tangent, 'r--', linewidth=2, label=f'Tangent at x={x0} (slope={slope})')\n",
    "ax.scatter([x0], [y0], color='red', s=100, zorder=5)\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-1, 9)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Derivative as Slope of Tangent Line')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Common Derivatives\n",
    "\n",
    "| Function | Derivative |\n",
    "|----------|------------|\n",
    "| $f(x) = c$ (constant) | $f'(x) = 0$ |\n",
    "| $f(x) = x^n$ | $f'(x) = nx^{n-1}$ |\n",
    "| $f(x) = e^x$ | $f'(x) = e^x$ |\n",
    "| $f(x) = \\ln(x)$ | $f'(x) = 1/x$ |\n",
    "| $f(x) = \\sigma(x) = \\frac{1}{1+e^{-x}}$ | $f'(x) = \\sigma(x)(1-\\sigma(x))$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 The Chain Rule\n",
    "\n",
    "The **chain rule** is the foundation of backpropagation. It tells us how to differentiate composite functions.\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx} = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "**Intuition:** Multiply the derivatives along the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: y = (2x + 1)^3\n",
    "# Let g(x) = 2x + 1, f(g) = g^3\n",
    "# dy/dx = df/dg * dg/dx = 3g^2 * 2 = 6(2x+1)^2\n",
    "\n",
    "def composite(x):\n",
    "    return (2*x + 1)**3\n",
    "\n",
    "def analytical_derivative(x):\n",
    "    return 6 * (2*x + 1)**2\n",
    "\n",
    "x = 2\n",
    "numerical = numerical_derivative(composite, x)\n",
    "analytical = analytical_derivative(x)\n",
    "\n",
    "print(f\"y = (2x + 1)^3 at x = {x}\")\n",
    "print(f\"Numerical: {numerical:.4f}\")\n",
    "print(f\"Analytical (chain rule): {analytical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "In a neural network:\n",
    "$$\\text{Loss} = L(\\sigma(wx + b))$$\n",
    "\n",
    "To find $\\frac{\\partial L}{\\partial w}$, we apply the chain rule:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\sigma} \\cdot \\frac{\\partial \\sigma}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "This is **backpropagation** - multiplying gradients backwards through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Partial Derivatives\n",
    "\n",
    "When a function has multiple variables, a **partial derivative** measures change with respect to one variable while holding others constant.\n",
    "\n",
    "$$f(x, y) = x^2 + xy + y^2$$\n",
    "$$\\frac{\\partial f}{\\partial x} = 2x + y$$\n",
    "$$\\frac{\\partial f}{\\partial y} = x + 2y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical partial derivatives\n",
    "def f(x, y):\n",
    "    return x**2 + x*y + y**2\n",
    "\n",
    "def partial_x(x, y, h=1e-5):\n",
    "    return (f(x+h, y) - f(x-h, y)) / (2*h)\n",
    "\n",
    "def partial_y(x, y, h=1e-5):\n",
    "    return (f(x, y+h) - f(x, y-h)) / (2*h)\n",
    "\n",
    "x, y = 2, 3\n",
    "\n",
    "print(f\"f(x, y) = x^2 + xy + y^2 at ({x}, {y})\")\n",
    "print(f\"df/dx numerical: {partial_x(x, y):.4f}, analytical (2x+y): {2*x + y}\")\n",
    "print(f\"df/dy numerical: {partial_y(x, y):.4f}, analytical (x+2y): {x + 2*y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 The Gradient\n",
    "\n",
    "The **gradient** is a vector of all partial derivatives. It points in the direction of steepest ascent.\n",
    "\n",
    "$$\\nabla f = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient on a 2D function\n",
    "x = np.linspace(-3, 3, 20)\n",
    "y = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2  # Simple bowl function\n",
    "\n",
    "# Gradients: df/dx = 2x, df/dy = 2y\n",
    "U = 2 * X\n",
    "V = 2 * Y\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "ax.quiver(X, Y, -U, -V, color='red', alpha=0.6)  # Negative gradient (descent)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Gradient Descent Direction on f(x,y) = x^2 + y^2\\nArrows show negative gradient (descent direction)')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "In gradient descent, we update parameters in the **opposite** direction of the gradient:\n",
    "$$\\theta_{new} = \\theta_{old} - \\alpha \\nabla L(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ are the model parameters\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla L$ is the gradient of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Probability Basics\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Probability Distributions\n",
    "\n",
    "A **probability distribution** describes how likely different outcomes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Uniform distribution\n",
    "uniform_samples = np.random.uniform(0, 1, 10000)\n",
    "axes[0].hist(uniform_samples, bins=50, density=True, alpha=0.7)\n",
    "axes[0].set_title('Uniform Distribution U(0, 1)')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# Normal (Gaussian) distribution\n",
    "normal_samples = np.random.normal(0, 1, 10000)\n",
    "axes[1].hist(normal_samples, bins=50, density=True, alpha=0.7)\n",
    "x = np.linspace(-4, 4, 100)\n",
    "axes[1].plot(x, 1/np.sqrt(2*np.pi) * np.exp(-x**2/2), 'r-', linewidth=2)\n",
    "axes[1].set_title('Normal Distribution N(0, 1)')\n",
    "axes[1].set_xlabel('Value')\n",
    "\n",
    "# Bernoulli (binary) outcomes\n",
    "p = 0.7\n",
    "bernoulli_samples = np.random.binomial(1, p, 10000)\n",
    "axes[2].bar([0, 1], [np.mean(bernoulli_samples==0), np.mean(bernoulli_samples==1)], \n",
    "            alpha=0.7, tick_label=['0', '1'])\n",
    "axes[2].set_title(f'Bernoulli Distribution (p={p})')\n",
    "axes[2].set_xlabel('Outcome')\n",
    "axes[2].set_ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "- **Normal distribution**: Used for weight initialization (randn)\n",
    "- **Uniform distribution**: Also for initialization (rand)\n",
    "- **Bernoulli**: Dropout (randomly zeroing neurons with probability p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Expected Value and Variance\n",
    "\n",
    "**Expected Value (Mean):** The average value we expect\n",
    "$$E[X] = \\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "**Variance:** How spread out values are from the mean\n",
    "$$Var(X) = \\sigma^2 = E[(X - \\mu)^2] = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2$$\n",
    "\n",
    "**Standard Deviation:** $\\sigma = \\sqrt{Var(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing statistics\n",
    "data = np.random.normal(loc=5, scale=2, size=10000)  # Mean=5, Std=2\n",
    "\n",
    "print(f\"Sample mean: {np.mean(data):.4f} (expected: 5)\")\n",
    "print(f\"Sample std: {np.std(data):.4f} (expected: 2)\")\n",
    "print(f\"Sample variance: {np.var(data):.4f} (expected: 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Context\n",
    "\n",
    "- **Batch Normalization**: Normalizes using batch mean and variance\n",
    "- **Weight Initialization**: Variance of weights affects gradient flow\n",
    "- **Standardization**: Input preprocessing to zero mean, unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Softmax and Probability\n",
    "\n",
    "The **softmax** function converts a vector of numbers into a probability distribution:\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "Properties:\n",
    "- All outputs are between 0 and 1\n",
    "- All outputs sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "# Raw scores (logits) from a classifier\n",
    "logits = np.array([2.0, 1.0, 0.5])\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Sum of probabilities: {np.sum(probs):.4f}\")\n",
    "print(f\"Predicted class: {np.argmax(probs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Key Points Summary\n",
    "\n",
    "---\n",
    "\n",
    "## NumPy Essentials\n",
    "- Arrays are the foundation - use vectorized operations, never loops\n",
    "- Broadcasting allows operations on different-shaped arrays\n",
    "- Reshaping is essential for matching layer dimensions\n",
    "\n",
    "## Linear Algebra\n",
    "- Dot product is the core operation: weighted sum of inputs\n",
    "- Matrix multiplication transforms data: (batch, in_features) @ (in_features, out_features) = (batch, out_features)\n",
    "- Norms measure size, used in regularization\n",
    "\n",
    "## Calculus\n",
    "- Derivatives measure rate of change - essential for optimization\n",
    "- Chain rule enables backpropagation: multiply gradients along the path\n",
    "- Gradient is a vector of partial derivatives pointing toward steepest ascent\n",
    "\n",
    "## Probability\n",
    "- Normal distribution used for initialization\n",
    "- Mean and variance used in normalization\n",
    "- Softmax converts logits to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Interview Tips\n",
    "\n",
    "---\n",
    "\n",
    "## Common Questions\n",
    "\n",
    "**Q: Why use vectorization instead of loops?**\n",
    "A: Vectorized operations use optimized C code and SIMD instructions, making them 10-100x faster than Python loops. This matters when training on millions of examples.\n",
    "\n",
    "**Q: Explain broadcasting.**\n",
    "A: Broadcasting automatically expands smaller arrays to match larger ones for element-wise operations. Rules: dimensions are compatible if equal or one is 1. It's how we efficiently add bias to a batch.\n",
    "\n",
    "**Q: What is the chain rule and why does it matter?**\n",
    "A: The chain rule lets us differentiate composite functions by multiplying derivatives along the path. It's the foundation of backpropagation - we compute gradients by propagating backwards through the network, multiplying local gradients.\n",
    "\n",
    "**Q: Why do we normalize inputs?**\n",
    "A: Normalization (zero mean, unit variance) helps optimization converge faster. Without it, features on different scales can cause the loss landscape to be elongated, making gradient descent oscillate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practice Exercises\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Matrix Dimensions\n",
    "\n",
    "Given:\n",
    "- Input X: (32, 784) - batch of 32 images, 784 features each\n",
    "- Weights W1: (256, 784) - first layer\n",
    "- Weights W2: (10, 256) - second layer\n",
    "\n",
    "What are the shapes after each operation?\n",
    "1. Z1 = X @ W1.T\n",
    "2. Z2 = Z1 @ W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# Z1 shape: ?\n",
    "# Z2 shape: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement Standardization\n",
    "\n",
    "Write a function to standardize an array (zero mean, unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize array to zero mean and unit variance.\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_data = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "result = standardize(test_data)\n",
    "print(f\"Mean after standardization: {np.mean(result):.6f} (should be ~0)\")\n",
    "print(f\"Std after standardization: {np.std(result):.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Derivative of Sigmoid\n",
    "\n",
    "The sigmoid function is:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Prove that its derivative is:\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "Then implement and verify numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Analytical derivative of sigmoid.\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Verify\n",
    "x = 2.0\n",
    "numerical = numerical_derivative(sigmoid, x)\n",
    "analytical = sigmoid_derivative(x)\n",
    "print(f\"Numerical: {numerical:.6f}\")\n",
    "print(f\"Analytical: {analytical:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "Run the cell below to see solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "print(\"Exercise 1:\")\n",
    "print(\"Z1 = X @ W1.T: (32, 784) @ (784, 256) = (32, 256)\")\n",
    "print(\"Z2 = Z1 @ W2.T: (32, 256) @ (256, 10) = (32, 10)\")\n",
    "\n",
    "# Exercise 2 Solution\n",
    "print(\"\\nExercise 2:\")\n",
    "def standardize_solution(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    return (x - mean) / std\n",
    "\n",
    "test_data = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "result = standardize_solution(test_data)\n",
    "print(f\"Standardized: {result}\")\n",
    "print(f\"Mean: {np.mean(result):.10f}, Std: {np.std(result):.6f}\")\n",
    "\n",
    "# Exercise 3 Solution\n",
    "print(\"\\nExercise 3:\")\n",
    "def sigmoid_derivative_solution(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"sigmoid'({x}) = {sigmoid_derivative_solution(x):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Module: [02 - Introduction to Deep Learning](../02_intro_to_deep_learning/02_intro_deep_learning.ipynb)\n",
    "\n",
    "In the next module, we will explore what deep learning is, how it differs from traditional machine learning, and why it has become so powerful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
