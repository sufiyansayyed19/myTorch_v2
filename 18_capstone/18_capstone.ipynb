{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJAL3bUWrB3t"
      },
      "source": [
        "# Module 18: Practice Problems & Interview Questions\n",
        "\n",
        "**Test Your Knowledge**\n",
        "\n",
        "---\n",
        "\n",
        "## Section 1: Conceptual Questions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn76I_-qrB3u"
      },
      "source": [
        "### Q1: What is the difference between a parameter and a hyperparameter?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "- **Parameters**: Learned during training (weights, biases)\n",
        "- **Hyperparameters**: Set before training (learning rate, batch size, number of layers)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2E7MasTrB3u"
      },
      "source": [
        "### Q2: Why do we use activation functions?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Activation functions introduce non-linearity. Without them, a neural network is just a linear transformation no matter how many layers it has. Non-linearity allows networks to learn complex patterns.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FI8ggHXrB3u"
      },
      "source": [
        "### Q3: What is vanishing gradient problem?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "During backpropagation, gradients can become extremely small when multiplied through many layers (especially with sigmoid/tanh). This prevents early layers from learning. Solutions: ReLU, residual connections, proper initialization, BatchNorm.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jlAN-DYrB3v"
      },
      "source": [
        "### Q4: Why is ReLU preferred over sigmoid?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "1. No vanishing gradient for positive values (gradient is 1)\n",
        "2. Computationally faster (no exp)\n",
        "3. Leads to sparse activations (some neurons output 0)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzfOYwMjrB3v"
      },
      "source": [
        "### Q5: What is the purpose of dropout?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Dropout is a regularization technique that randomly sets some activations to zero during training. This prevents co-adaptation of neurons and forces the network to learn more robust features. It's disabled during inference.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hoDHowrrB3v"
      },
      "source": [
        "### Q6: What is batch normalization and why does it help?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "BatchNorm normalizes activations to have zero mean and unit variance, then learns to scale and shift. It:\n",
        "- Stabilizes training\n",
        "- Allows higher learning rates\n",
        "- Reduces sensitivity to initialization\n",
        "- Acts as regularization\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9po1C9ErB3v"
      },
      "source": [
        "### Q7: Explain the difference between SGD, SGD with momentum, and Adam.\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "- **SGD**: Updates weights using gradient alone\n",
        "- **SGD + Momentum**: Accumulates velocity, smooths updates, escapes local minima\n",
        "- **Adam**: Combines momentum with adaptive learning rates per parameter. Usually works well out of the box.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AbO5fbarB3v"
      },
      "source": [
        "### Q8: Why do we need weight initialization?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Proper initialization:\n",
        "- Prevents vanishing/exploding gradients\n",
        "- Ensures activations have reasonable variance across layers\n",
        "- Xavier for tanh/sigmoid, He for ReLU\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zARZX9EqrB3v"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Coding Exercises\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgNDhtsJrB3w"
      },
      "source": [
        "### Exercise 1: Implement Sigmoid from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kYZprYOarB3w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test\n",
        "x = torch.tensor([0., 1., -1.])\n",
        "# Expected: [0.5, 0.7311, 0.2689]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiNxXYfprB3w"
      },
      "source": [
        "### Exercise 2: Implement Binary Cross-Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-vKzVHLUrB3w"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test\n",
        "y_true = np.array([1, 0, 1])\n",
        "y_pred = np.array([0.9, 0.1, 0.8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_HbXR-nrB3w"
      },
      "source": [
        "### Exercise 3: Build a 3-Layer MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R5lDlT6qrB3w"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ThreeLayerMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden1, hidden2, output_size):\n",
        "        super().__init__()\n",
        "        # Your code here\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code here\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_KjR1I2rB3w"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: Solutions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hwI9_yFrB3w",
        "outputId": "14b8f32f-87f3-4c93-d07b-a1926f4d2e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: tensor([0.5000, 0.7311, 0.2689])\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1 Solution\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "x = torch.tensor([0., 1., -1.])\n",
        "print(f\"Sigmoid: {sigmoid(x)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Teckz9bdrB3w",
        "outputId": "b1c1de5a-1cde-49d7-a867-d3d647696b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCE: 0.1446\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2 Solution\n",
        "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "y_true = np.array([1, 0, 1])\n",
        "y_pred = np.array([0.9, 0.1, 0.8])\n",
        "print(f\"BCE: {binary_cross_entropy(y_true, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8QhRrQKrB3x",
        "outputId": "4e8f22d4-f7b2-4f60-871c-af921a33a1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 235,146\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3 Solution\n",
        "class ThreeLayerMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden1, hidden2, output_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ThreeLayerMLP(784, 256, 128, 10)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tP3TskqrB3x"
      },
      "source": [
        "---\n",
        "\n",
        "# Congratulations! ðŸŽ‰\n",
        "\n",
        "You've completed the PyTorch Deep Learning Fundamentals course!\n",
        "\n",
        "## Topics Covered:\n",
        "1. Math Prerequisites\n",
        "2. Introduction to Deep Learning\n",
        "3. PyTorch Fundamentals\n",
        "4. The Neuron\n",
        "5. Activation Functions\n",
        "6. Perceptron\n",
        "7. Loss Functions\n",
        "8. Gradient Descent\n",
        "9. Backpropagation\n",
        "10. Optimizers\n",
        "11. Building Neural Networks\n",
        "12. Training Pipeline\n",
        "13. Regularization\n",
        "14. Weight Initialization\n",
        "15. Batch Normalization\n",
        "16. Hyperparameter Tuning\n",
        "17. Practical Project\n",
        "18. Interview Prep\n",
        "\n",
        "## Next Steps:\n",
        "- Practice with real datasets (MNIST, CIFAR-10)\n",
        "- Explore CNNs for image tasks\n",
        "- Learn RNNs/Transformers for sequences\n",
        "- Build projects!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}