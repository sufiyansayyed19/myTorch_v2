{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE13zP5s26by"
      },
      "source": [
        "# Module 11: Building Neural Networks in PyTorch\n",
        "\n",
        "**From Scratch to Production-Ready Models**\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Master the nn.Module class\n",
        "- Build networks with nn.Linear and nn.Sequential\n",
        "- Understand the forward method\n",
        "- Work with parameters and buffers\n",
        "- Create custom layers\n",
        "\n",
        "**Prerequisites:** [Module 03 - PyTorch Fundamentals](../03_pytorch_fundamentals/03_pytorch_fundamentals.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJuvoVut26b0",
        "outputId": "2bef3354-0780-49ad-bf0d-7eacd8e03366"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e1c24016070>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RhjCCoq26b0"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 1: nn.Module - The Building Block\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 What is nn.Module?\n",
        "\n",
        "`nn.Module` is the base class for all neural network modules in PyTorch. It provides:\n",
        "- Parameter management\n",
        "- GPU transfer\n",
        "- Saving/loading\n",
        "- Training/eval modes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8qz3SSc26b0",
        "outputId": "c56fc223-40f1-41b8-e775-3b82545dd253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3])\n",
            "Output shape: torch.Size([4, 2])\n"
          ]
        }
      ],
      "source": [
        "# Simplest possible module\n",
        "class SimpleLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()  # ALWAYS call super().__init__()\n",
        "\n",
        "        # Define learnable parameters using nn.Parameter\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        return x @ self.weight.T + self.bias\n",
        "\n",
        "# Test\n",
        "layer = SimpleLinear(3, 2)\n",
        "x = torch.randn(4, 3)  # Batch of 4 samples, 3 features each\n",
        "output = layer(x)  # Calls forward() automatically\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIGrh0oy26b1",
        "outputId": "968cfcab-9618-4451-d138-0829b1bab98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "  weight: shape=torch.Size([2, 3]), requires_grad=True\n",
            "  bias: shape=torch.Size([2]), requires_grad=True\n"
          ]
        }
      ],
      "source": [
        "# Inspect parameters\n",
        "print(\"Parameters:\")\n",
        "for name, param in layer.named_parameters():\n",
        "    print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeLaQG9j26b1"
      },
      "source": [
        "## 1.2 Using nn.Linear\n",
        "\n",
        "PyTorch provides `nn.Linear` which does the same thing but with proper initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1EJSzLa26b1",
        "outputId": "7a1ec0ea-2d9a-4e52-ecdd-b6b87f53fe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight shape: torch.Size([2, 3])\n",
            "Bias shape: torch.Size([2])\n",
            "\n",
            "Without bias: bias = None\n"
          ]
        }
      ],
      "source": [
        "# nn.Linear is the standard way\n",
        "linear = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "print(f\"Weight shape: {linear.weight.shape}\")\n",
        "print(f\"Bias shape: {linear.bias.shape}\")\n",
        "\n",
        "# Without bias\n",
        "linear_no_bias = nn.Linear(3, 2, bias=False)\n",
        "print(f\"\\nWithout bias: bias = {linear_no_bias.bias}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFRNeTLw26b1"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 2: Building Networks\n",
        "\n",
        "---\n",
        "\n",
        "## 2.1 Class-Based Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC30y3UC26b1",
        "outputId": "fca7613c-3ba3-49fc-fa0e-634949c11725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([32, 784])\n",
            "Output shape: torch.Size([32, 10])\n",
            "\n",
            "Total parameters: 118,282\n"
          ]
        }
      ],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-Layer Perceptron with configurable layers.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Activation functions (not layers, no parameters)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Output layer (no activation for logits)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create and test\n",
        "model = MLP(input_size=784, hidden_size=128, output_size=10)\n",
        "x = torch.randn(32, 784)  # Batch of 32 images (flattened 28x28)\n",
        "output = model(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PoiLTnB26b1"
      },
      "source": [
        "## 2.2 Using nn.Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br902Rg626b1",
        "outputId": "82f235a8-560f-43ed-c669-8b961189c20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([32, 10])\n",
            "\n",
            "First layer: Linear(in_features=784, out_features=128, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# nn.Sequential: Quick way to build simple feedforward networks\n",
        "model_seq = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "output = model_seq(x)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Access layers by index\n",
        "print(f\"\\nFirst layer: {model_seq[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh4y-C1L26b2",
        "outputId": "f3061bdd-45ae-432a-ac98-1f733fb5853c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access by name: Linear(in_features=784, out_features=128, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# Named Sequential with OrderedDict\n",
        "from collections import OrderedDict\n",
        "\n",
        "model_named = nn.Sequential(OrderedDict([\n",
        "    ('fc1', nn.Linear(784, 128)),\n",
        "    ('relu1', nn.ReLU()),\n",
        "    ('fc2', nn.Linear(128, 128)),\n",
        "    ('relu2', nn.ReLU()),\n",
        "    ('fc3', nn.Linear(128, 10))\n",
        "]))\n",
        "\n",
        "# Access by name\n",
        "print(f\"Access by name: {model_named.fc1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFc_7ePI26b2"
      },
      "source": [
        "## 2.3 Hybrid Approach (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSwdKBnH26b2",
        "outputId": "b188656f-6723-43e6-a62c-723a4159dbdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shallow: 101,770 params\n",
            "Deep: 242,762 params\n"
          ]
        }
      ],
      "source": [
        "class FlexibleMLP(nn.Module):\n",
        "    \"\"\"MLP with variable number of hidden layers.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build layers dynamically\n",
        "        layers = []\n",
        "        in_size = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            in_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(in_size, output_size))\n",
        "\n",
        "        # Wrap in Sequential\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Create networks with different architectures\n",
        "model_shallow = FlexibleMLP(784, [128], 10)\n",
        "model_deep = FlexibleMLP(784, [256, 128, 64], 10, dropout=0.2)\n",
        "\n",
        "print(f\"Shallow: {sum(p.numel() for p in model_shallow.parameters()):,} params\")\n",
        "print(f\"Deep: {sum(p.numel() for p in model_deep.parameters()):,} params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVd-I6lf26b2"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3: Parameters and Buffers\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 Parameters vs Buffers\n",
        "\n",
        "- **Parameters**: Learnable weights (have gradients)\n",
        "- **Buffers**: Non-learnable state (e.g., running mean in BatchNorm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-CGxX1h26b2",
        "outputId": "5506a060-b487-40a6-a4ce-8e159efed5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "  weight\n",
            "\n",
            "Buffers:\n",
            "  running_mean\n",
            "  count\n"
          ]
        }
      ],
      "source": [
        "class LayerWithBuffer(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Learnable parameter\n",
        "        self.weight = nn.Parameter(torch.randn(size))\n",
        "\n",
        "        # Non-learnable buffer (still saved/loaded, moves to GPU)\n",
        "        self.register_buffer('running_mean', torch.zeros(size))\n",
        "        self.register_buffer('count', torch.tensor(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Update running mean during training\n",
        "        if self.training:\n",
        "            self.running_mean = 0.9 * self.running_mean + 0.1 * x.mean()\n",
        "            self.count += 1\n",
        "        return x * self.weight\n",
        "\n",
        "layer = LayerWithBuffer(3)\n",
        "print(\"Parameters:\")\n",
        "for name, p in layer.named_parameters():\n",
        "    print(f\"  {name}\")\n",
        "\n",
        "print(\"\\nBuffers:\")\n",
        "for name, b in layer.named_buffers():\n",
        "    print(f\"  {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVME12oU26b2"
      },
      "source": [
        "## 3.2 Training vs Evaluation Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y0v40lR26b2",
        "outputId": "b61e3d7a-8f03-47c2-80c5-adb2cf83bc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mode: True\n",
            "Training mode: False\n"
          ]
        }
      ],
      "source": [
        "model = FlexibleMLP(784, [128], 10, dropout=0.5)\n",
        "\n",
        "# Training mode (default)\n",
        "model.train()\n",
        "print(f\"Training mode: {model.training}\")\n",
        "\n",
        "# Evaluation mode\n",
        "model.eval()\n",
        "print(f\"Training mode: {model.training}\")\n",
        "\n",
        "# This affects:\n",
        "# - Dropout: disabled in eval mode\n",
        "# - BatchNorm: uses running stats in eval mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHiWPwsF26b2"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 4: Custom Layers\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRyIRtDr26b2",
        "outputId": "3da36ac9-611b-4ff0-b727-6400e1f55f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 200,842\n"
          ]
        }
      ],
      "source": [
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish activation: x * sigmoid(x)\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block: output = F(x) + x\"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(size, size)\n",
        "        self.fc2 = nn.Linear(size, size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = out + identity  # Skip connection!\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Use in a network\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_blocks=3):\n",
        "        super().__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_size) for _ in range(n_blocks)])\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "model = ResNet(784, 128, 10, n_blocks=3)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2hhyyv26b2"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 5: Useful Patterns\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6FWw4zw26b2",
        "outputId": "45ae2db3-43f4-4947-ba31-ada97afde94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 200,842\n"
          ]
        }
      ],
      "source": [
        "# Moving to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Saving and loading\n",
        "# Save\n",
        "# torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "# Load\n",
        "# model.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Trainable parameters: {count_parameters(model):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs43IxHj26b3"
      },
      "source": [
        "---\n",
        "\n",
        "# Key Points Summary\n",
        "\n",
        "---\n",
        "\n",
        "## nn.Module\n",
        "- Base class for all neural networks\n",
        "- Always call `super().__init__()` in constructor\n",
        "- Define layers in `__init__`, computation in `forward`\n",
        "\n",
        "## Building Networks\n",
        "- Use `nn.Linear`, `nn.ReLU`, etc. for standard layers\n",
        "- Use `nn.Sequential` for simple feedforward networks\n",
        "- Use class-based approach for complex architectures\n",
        "\n",
        "## Parameters\n",
        "- Use `nn.Parameter` for learnable weights\n",
        "- Use `register_buffer` for non-learnable state\n",
        "- `model.train()` and `model.eval()` for mode switching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLAd8TtU26b3"
      },
      "source": [
        "---\n",
        "\n",
        "# Interview Tips\n",
        "\n",
        "---\n",
        "\n",
        "**Q: What is nn.Module?**\n",
        "A: The base class for all neural networks in PyTorch. It handles parameter registration, GPU transfer, serialization, and training/eval modes.\n",
        "\n",
        "**Q: When to use nn.Sequential vs class-based?**\n",
        "A: Use Sequential for simple feedforward networks. Use class-based for custom logic like skip connections, multiple outputs, or conditional execution.\n",
        "\n",
        "**Q: What's the difference between parameters and buffers?**\n",
        "A: Parameters are learnable (have gradients). Buffers are non-learnable state that should still be saved/loaded (e.g., running statistics in BatchNorm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJ4PjOW26b3"
      },
      "source": [
        "---\n",
        "\n",
        "## Next Module: [12 - Training Pipeline](../12_training_pipeline/12_training_pipeline.ipynb)\n",
        "\n",
        "Now let's learn how to properly train these networks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}