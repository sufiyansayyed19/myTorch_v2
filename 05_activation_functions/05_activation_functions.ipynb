{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 05: Activation Functions\n",
                "\n",
                "**The Key to Non-Linearity**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "- Understand why activation functions are essential\n",
                "- Master sigmoid, tanh, ReLU, and their variants\n",
                "- Know the problems with each activation (vanishing gradients, dying ReLU)\n",
                "- Understand softmax for classification\n",
                "- Know when to use which activation function\n",
                "\n",
                "**Prerequisites:** [Module 04 - The Neuron](../04_the_neuron/04_neuron.ipynb)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: Why Non-Linearity?\n",
                "\n",
                "---\n",
                "\n",
                "## 1.1 The Problem with Linear Functions\n",
                "\n",
                "Without activation functions, a neural network is just a linear transformation.\n",
                "\n",
                "**Proof:** Consider a 2-layer network without activations:\n",
                "- Layer 1: $h = W_1 x + b_1$\n",
                "- Layer 2: $y = W_2 h + b_2 = W_2 (W_1 x + b_1) + b_2$\n",
                "\n",
                "Simplifying:\n",
                "$$y = W_2 W_1 x + W_2 b_1 + b_2 = W' x + b'$$\n",
                "\n",
                "This is just another linear function! No matter how many layers we add, the result is always linear."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration: Stacking linear layers without activation\n",
                "torch.manual_seed(42)\n",
                "\n",
                "# 3-layer \"network\" without activations\n",
                "layer1 = nn.Linear(2, 4)\n",
                "layer2 = nn.Linear(4, 4)\n",
                "layer3 = nn.Linear(4, 1)\n",
                "\n",
                "# The effective transformation\n",
                "W_combined = layer3.weight @ layer2.weight @ layer1.weight\n",
                "print(f\"Layer 1 weight shape: {layer1.weight.shape}\")\n",
                "print(f\"Layer 2 weight shape: {layer2.weight.shape}\")\n",
                "print(f\"Layer 3 weight shape: {layer3.weight.shape}\")\n",
                "print(f\"\\nCombined weight shape: {W_combined.shape}\")\n",
                "print(\"\\nA 3-layer network without activations = single linear transform!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Linear vs Non-linear\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Linear function\n",
                "x = np.linspace(-5, 5, 100)\n",
                "y_linear = 2 * x + 1\n",
                "\n",
                "axes[0].plot(x, y_linear, 'b-', linewidth=2)\n",
                "axes[0].set_title('Linear Function: y = 2x + 1\\n(Limited representation)')\n",
                "axes[0].set_xlabel('x')\n",
                "axes[0].set_ylabel('y')\n",
                "\n",
                "# Non-linear function (what we can learn with activations)\n",
                "y_nonlinear = np.sin(x) + 0.5 * np.cos(2*x)\n",
                "axes[1].plot(x, y_nonlinear, 'r-', linewidth=2)\n",
                "axes[1].set_title('Non-linear Function\\n(Rich representation)')\n",
                "axes[1].set_xlabel('x')\n",
                "axes[1].set_ylabel('y')\n",
                "\n",
                "plt.suptitle('Why We Need Non-Linearity', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 What Activation Functions Do\n",
                "\n",
                "Activation functions:\n",
                "1. **Introduce non-linearity** - Allow learning complex patterns\n",
                "2. **Control output range** - Bound outputs to useful ranges\n",
                "3. **Enable gradient flow** - Their derivatives are used in backpropagation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: Sigmoid\n",
                "\n",
                "---\n",
                "\n",
                "## 2.1 Definition and Properties\n",
                "\n",
                "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
                "\n",
                "**Properties:**\n",
                "- Output range: (0, 1)\n",
                "- Monotonically increasing\n",
                "- Differentiable everywhere\n",
                "- $\\sigma(0) = 0.5$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "def sigmoid_derivative(z):\n",
                "    s = sigmoid(z)\n",
                "    return s * (1 - s)\n",
                "\n",
                "# Plot sigmoid and its derivative\n",
                "z = np.linspace(-10, 10, 200)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Sigmoid\n",
                "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
                "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[0].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
                "axes[0].axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
                "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[0].set_xlabel('z')\n",
                "axes[0].set_ylabel('sigma(z)')\n",
                "axes[0].set_title('Sigmoid Function')\n",
                "axes[0].set_ylim(-0.1, 1.1)\n",
                "\n",
                "# Derivative\n",
                "axes[1].plot(z, sigmoid_derivative(z), 'r-', linewidth=2)\n",
                "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[1].axhline(y=0.25, color='gray', linestyle='--', alpha=0.5, label='max = 0.25')\n",
                "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[1].set_xlabel('z')\n",
                "axes[1].set_ylabel(\"sigma'(z)\")\n",
                "axes[1].set_title('Sigmoid Derivative')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 The Vanishing Gradient Problem\n",
                "\n",
                "Notice the derivative:\n",
                "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
                "\n",
                "**Maximum value is 0.25** (when z=0). This causes problems:\n",
                "- In backprop, we multiply many derivatives\n",
                "- If each is at most 0.25, the gradient shrinks exponentially\n",
                "- Deep layers receive almost zero gradient"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration: Gradient shrinking through layers\n",
                "n_layers = 10\n",
                "gradient = 1.0  # Initial gradient\n",
                "max_derivative = 0.25  # Maximum sigmoid derivative\n",
                "\n",
                "gradients = [gradient]\n",
                "for _ in range(n_layers):\n",
                "    gradient *= max_derivative\n",
                "    gradients.append(gradient)\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.bar(range(n_layers + 1), gradients)\n",
                "plt.xlabel('Layer (from output to input)')\n",
                "plt.ylabel('Gradient magnitude')\n",
                "plt.title('Vanishing Gradient with Sigmoid\\nGradient shrinks by at least 4x per layer!')\n",
                "plt.yscale('log')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(f\"After 10 layers, gradient is multiplied by at most: {0.25**10:.2e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Sigmoid is Not Zero-Centered\n",
                "\n",
                "Sigmoid outputs are always positive (0 to 1). This causes:\n",
                "- All gradients for weights have the same sign\n",
                "- Updates can only be all positive or all negative\n",
                "- Leads to \"zig-zagging\" in optimization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 When to Use Sigmoid\n",
                "\n",
                "**Use sigmoid:**\n",
                "- Output layer for **binary classification** (probability between 0 and 1)\n",
                "- Gates in LSTM/GRU (controls information flow)\n",
                "\n",
                "**Avoid sigmoid:**\n",
                "- Hidden layers (use ReLU instead)\n",
                "- Deep networks (vanishing gradients)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: Tanh (Hyperbolic Tangent)\n",
                "\n",
                "---\n",
                "\n",
                "## 3.1 Definition and Properties\n",
                "\n",
                "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1$$\n",
                "\n",
                "**Properties:**\n",
                "- Output range: (-1, 1)\n",
                "- **Zero-centered** (unlike sigmoid)\n",
                "- $\\tanh(0) = 0$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def tanh(z):\n",
                "    return np.tanh(z)\n",
                "\n",
                "def tanh_derivative(z):\n",
                "    return 1 - np.tanh(z)**2\n",
                "\n",
                "# Compare sigmoid and tanh\n",
                "z = np.linspace(-5, 5, 200)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Functions\n",
                "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2, label='Sigmoid')\n",
                "axes[0].plot(z, tanh(z), 'r-', linewidth=2, label='Tanh')\n",
                "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[0].set_xlabel('z')\n",
                "axes[0].set_ylabel('output')\n",
                "axes[0].set_title('Sigmoid vs Tanh')\n",
                "axes[0].legend()\n",
                "\n",
                "# Derivatives\n",
                "axes[1].plot(z, sigmoid_derivative(z), 'b-', linewidth=2, label=\"Sigmoid' (max=0.25)\")\n",
                "axes[1].plot(z, tanh_derivative(z), 'r-', linewidth=2, label=\"Tanh' (max=1.0)\")\n",
                "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[1].set_xlabel('z')\n",
                "axes[1].set_ylabel('derivative')\n",
                "axes[1].set_title('Derivatives Comparison')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Tanh vs Sigmoid\n",
                "\n",
                "| Property | Sigmoid | Tanh |\n",
                "|----------|---------|------|\n",
                "| Range | (0, 1) | (-1, 1) |\n",
                "| Zero-centered | No | Yes |\n",
                "| Max derivative | 0.25 | 1.0 |\n",
                "| Vanishing gradient | Severe | Less severe |\n",
                "\n",
                "Tanh is generally preferred over sigmoid for hidden layers because it's zero-centered and has stronger gradients. But both still suffer from vanishing gradients in deep networks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: ReLU (Rectified Linear Unit)\n",
                "\n",
                "---\n",
                "\n",
                "## 4.1 Definition and Properties\n",
                "\n",
                "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
                "\n",
                "**Properties:**\n",
                "- Output range: [0, infinity)\n",
                "- Non-saturating for positive values\n",
                "- Computationally efficient\n",
                "- Sparse activation (many zeros)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def relu(z):\n",
                "    return np.maximum(0, z)\n",
                "\n",
                "def relu_derivative(z):\n",
                "    return (z > 0).astype(float)\n",
                "\n",
                "# Plot ReLU\n",
                "z = np.linspace(-5, 5, 200)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# ReLU\n",
                "axes[0].plot(z, relu(z), 'g-', linewidth=2)\n",
                "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[0].set_xlabel('z')\n",
                "axes[0].set_ylabel('ReLU(z)')\n",
                "axes[0].set_title('ReLU Function')\n",
                "\n",
                "# Derivative\n",
                "axes[1].plot(z, relu_derivative(z), 'g-', linewidth=2)\n",
                "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
                "axes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
                "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
                "axes[1].set_xlabel('z')\n",
                "axes[1].set_ylabel(\"ReLU'(z)\")\n",
                "axes[1].set_title('ReLU Derivative\\n(0 for z<0, 1 for z>0)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 Why ReLU is Popular\n",
                "\n",
                "1. **No vanishing gradient (for positive values)**\n",
                "   - Derivative is 1 for z > 0\n",
                "   - Gradients flow unchanged\n",
                "\n",
                "2. **Computationally efficient**\n",
                "   - Just a threshold (no exp)\n",
                "   - Fast forward and backward pass\n",
                "\n",
                "3. **Sparse activations**\n",
                "   - Many neurons output 0\n",
                "   - Acts as regularization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.3 The Dying ReLU Problem\n",
                "\n",
                "If a neuron's inputs always yield z < 0:\n",
                "- Output is always 0\n",
                "- Gradient is always 0\n",
                "- The neuron \"dies\" - never learns again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstration: Dying ReLU\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "z = np.linspace(-5, 5, 200)\n",
                "\n",
                "ax.plot(z, relu(z), 'g-', linewidth=2, label='ReLU output')\n",
                "ax.fill_between(z[z < 0], 0, -0.5, alpha=0.3, color='red', label='Dead zone (no gradients)')\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "\n",
                "ax.annotate('If inputs land here,\\nneuron is \"dead\"', \n",
                "            xy=(-2.5, 0), xytext=(-3, 2),\n",
                "            arrowprops=dict(arrowstyle='->', color='red'),\n",
                "            fontsize=10, color='red')\n",
                "\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('The Dying ReLU Problem')\n",
                "ax.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 5: ReLU Variants\n",
                "\n",
                "---\n",
                "\n",
                "## 5.1 Leaky ReLU\n",
                "\n",
                "$$\\text{LeakyReLU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha z & \\text{if } z \\leq 0 \\end{cases}$$\n",
                "\n",
                "Typically $\\alpha = 0.01$. Allows small gradient when z < 0."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def leaky_relu(z, alpha=0.01):\n",
                "    return np.where(z > 0, z, alpha * z)\n",
                "\n",
                "# Compare ReLU variants\n",
                "z = np.linspace(-5, 5, 200)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.plot(z, relu(z), 'g-', linewidth=2, label='ReLU')\n",
                "ax.plot(z, leaky_relu(z, 0.1), 'b-', linewidth=2, label='Leaky ReLU (alpha=0.1)')\n",
                "ax.plot(z, leaky_relu(z, 0.3), 'r--', linewidth=2, label='Leaky ReLU (alpha=0.3)')\n",
                "\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('ReLU vs Leaky ReLU')\n",
                "ax.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 ELU (Exponential Linear Unit)\n",
                "\n",
                "$$\\text{ELU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha (e^z - 1) & \\text{if } z \\leq 0 \\end{cases}$$\n",
                "\n",
                "- Smooth curve for negative values\n",
                "- Mean activations closer to zero"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def elu(z, alpha=1.0):\n",
                "    return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.plot(z, relu(z), 'g-', linewidth=2, label='ReLU')\n",
                "ax.plot(z, elu(z), 'purple', linewidth=2, label='ELU')\n",
                "\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('ReLU vs ELU')\n",
                "ax.legend()\n",
                "ax.set_ylim(-2, 5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.3 GELU (Gaussian Error Linear Unit)\n",
                "\n",
                "$$\\text{GELU}(z) = z \\cdot \\Phi(z)$$\n",
                "\n",
                "Where $\\Phi$ is the CDF of standard normal distribution.\n",
                "\n",
                "- Used in BERT, GPT, and modern transformers\n",
                "- Smooth approximation of ReLU\n",
                "- Allows small negative values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.special import erf\n",
                "\n",
                "def gelu(z):\n",
                "    return 0.5 * z * (1 + erf(z / np.sqrt(2)))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.plot(z, relu(z), 'g-', linewidth=2, label='ReLU')\n",
                "ax.plot(z, gelu(z), 'orange', linewidth=2, label='GELU')\n",
                "\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('ReLU vs GELU\\n(GELU is used in Transformers)')\n",
                "ax.legend()\n",
                "ax.set_ylim(-1, 5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.4 Swish\n",
                "\n",
                "$$\\text{Swish}(z) = z \\cdot \\sigma(z) = \\frac{z}{1 + e^{-z}}$$\n",
                "\n",
                "- Self-gated activation\n",
                "- Smooth and non-monotonic\n",
                "- Used in EfficientNet and some modern architectures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def swish(z):\n",
                "    return z * sigmoid(z)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "\n",
                "ax.plot(z, relu(z), 'g-', linewidth=2, label='ReLU')\n",
                "ax.plot(z, swish(z), 'brown', linewidth=2, label='Swish')\n",
                "\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('ReLU vs Swish')\n",
                "ax.legend()\n",
                "ax.set_ylim(-1, 5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.5 Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# All activations together\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "ax.plot(z, sigmoid(z), linewidth=2, label='Sigmoid')\n",
                "ax.plot(z, tanh(z), linewidth=2, label='Tanh')\n",
                "ax.plot(z, relu(z), linewidth=2, label='ReLU')\n",
                "ax.plot(z, leaky_relu(z, 0.1), linewidth=2, label='Leaky ReLU')\n",
                "ax.plot(z, elu(z), linewidth=2, label='ELU')\n",
                "ax.plot(z, gelu(z), linewidth=2, label='GELU')\n",
                "ax.plot(z, swish(z), linewidth=2, label='Swish')\n",
                "\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z', fontsize=12)\n",
                "ax.set_ylabel('output', fontsize=12)\n",
                "ax.set_title('Comparison of Activation Functions', fontsize=14)\n",
                "ax.legend(loc='upper left')\n",
                "ax.set_xlim(-5, 5)\n",
                "ax.set_ylim(-2, 5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 6: Softmax\n",
                "\n",
                "---\n",
                "\n",
                "## 6.1 Definition and Purpose\n",
                "\n",
                "For multi-class classification, we need outputs that:\n",
                "1. Are between 0 and 1 (probabilities)\n",
                "2. Sum to 1 (probability distribution)\n",
                "\n",
                "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(z):\n",
                "    exp_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n",
                "    return exp_z / np.sum(exp_z)\n",
                "\n",
                "# Example: 3-class classification\n",
                "logits = np.array([2.0, 1.0, 0.5])\n",
                "probs = softmax(logits)\n",
                "\n",
                "print(f\"Logits (raw scores): {logits}\")\n",
                "print(f\"Probabilities: {probs}\")\n",
                "print(f\"Sum of probabilities: {probs.sum():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "classes = ['Class 0', 'Class 1', 'Class 2']\n",
                "\n",
                "# Logits\n",
                "axes[0].bar(classes, logits, color='steelblue')\n",
                "axes[0].set_ylabel('Score')\n",
                "axes[0].set_title('Raw Logits (Before Softmax)')\n",
                "\n",
                "# Probabilities\n",
                "axes[1].bar(classes, probs, color='forestgreen')\n",
                "axes[1].set_ylabel('Probability')\n",
                "axes[1].set_title('Probabilities (After Softmax)')\n",
                "axes[1].set_ylim(0, 1)\n",
                "\n",
                "for ax, vals in zip(axes, [logits, probs]):\n",
                "    for i, v in enumerate(vals):\n",
                "        ax.text(i, v + 0.05, f'{v:.2f}', ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.2 Temperature in Softmax\n",
                "\n",
                "$$\\text{softmax}(z_i, T) = \\frac{e^{z_i/T}}{\\sum_{j=1}^{K} e^{z_j/T}}$$\n",
                "\n",
                "- **High T**: Softer distribution (more uniform)\n",
                "- **Low T**: Sharper distribution (more confident)\n",
                "- **T -> 0**: Becomes argmax"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax_temp(z, T=1.0):\n",
                "    z_scaled = z / T\n",
                "    exp_z = np.exp(z_scaled - np.max(z_scaled))\n",
                "    return exp_z / np.sum(exp_z)\n",
                "\n",
                "logits = np.array([2.0, 1.0, 0.5])\n",
                "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
                "\n",
                "for ax, T in zip(axes, temperatures):\n",
                "    probs = softmax_temp(logits, T)\n",
                "    ax.bar(classes, probs, color='steelblue')\n",
                "    ax.set_ylim(0, 1)\n",
                "    ax.set_title(f'T = {T}')\n",
                "    for i, v in enumerate(probs):\n",
                "        ax.text(i, v + 0.02, f'{v:.2f}', ha='center', fontsize=9)\n",
                "\n",
                "plt.suptitle('Effect of Temperature on Softmax', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 7: PyTorch Implementation\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch activation functions\n",
                "x = torch.linspace(-3, 3, 7)\n",
                "print(f\"Input: {x}\")\n",
                "\n",
                "# Module versions\n",
                "print(f\"\\nnn.Sigmoid: {nn.Sigmoid()(x)}\")\n",
                "print(f\"nn.Tanh: {nn.Tanh()(x)}\")\n",
                "print(f\"nn.ReLU: {nn.ReLU()(x)}\")\n",
                "print(f\"nn.LeakyReLU: {nn.LeakyReLU(0.1)(x)}\")\n",
                "print(f\"nn.ELU: {nn.ELU()(x)}\")\n",
                "print(f\"nn.GELU: {nn.GELU()(x)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Functional versions\n",
                "import torch.nn.functional as F\n",
                "\n",
                "print(f\"F.sigmoid: {F.sigmoid(x)}\")\n",
                "print(f\"F.relu: {F.relu(x)}\")\n",
                "print(f\"F.softmax: {F.softmax(x, dim=0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# In a neural network\n",
                "class SimpleNetwork(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size, activation='relu'):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
                "        \n",
                "        # Choose activation\n",
                "        if activation == 'relu':\n",
                "            self.activation = nn.ReLU()\n",
                "        elif activation == 'sigmoid':\n",
                "            self.activation = nn.Sigmoid()\n",
                "        elif activation == 'tanh':\n",
                "            self.activation = nn.Tanh()\n",
                "        elif activation == 'leaky_relu':\n",
                "            self.activation = nn.LeakyReLU(0.1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.fc1(x)\n",
                "        x = self.activation(x)  # Hidden layer activation\n",
                "        x = self.fc2(x)\n",
                "        return x  # No activation on output (for classification, use CrossEntropyLoss)\n",
                "\n",
                "# Test\n",
                "model = SimpleNetwork(10, 5, 3, activation='relu')\n",
                "x = torch.randn(2, 10)\n",
                "output = model(x)\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 8: Choosing the Right Activation\n",
                "\n",
                "---\n",
                "\n",
                "## 8.1 Quick Reference Guide\n",
                "\n",
                "| Location | Task | Recommended Activation |\n",
                "|----------|------|------------------------|\n",
                "| Hidden layers | General | ReLU |\n",
                "| Hidden layers | Potential dead neurons | Leaky ReLU |\n",
                "| Hidden layers | Transformers | GELU |\n",
                "| Output | Binary classification | Sigmoid |\n",
                "| Output | Multi-class classification | Softmax (or none with CrossEntropyLoss) |\n",
                "| Output | Regression | None (linear) |\n",
                "| Output | Bounded regression [0,1] | Sigmoid |\n",
                "| Output | Bounded regression [-1,1] | Tanh |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8.2 Common Mistakes to Avoid\n",
                "\n",
                "1. **Using sigmoid/tanh in hidden layers of deep networks**\n",
                "   - Causes vanishing gradients\n",
                "   - Use ReLU instead\n",
                "\n",
                "2. **Applying softmax before CrossEntropyLoss**\n",
                "   - CrossEntropyLoss includes softmax internally\n",
                "   - Applying it twice causes numerical issues\n",
                "\n",
                "3. **Using ReLU as output activation**\n",
                "   - For classification, use sigmoid/softmax\n",
                "   - For regression, often use no activation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Key Points Summary\n",
                "\n",
                "---\n",
                "\n",
                "## Why Activation Functions\n",
                "- Enable non-linearity\n",
                "- Without them, deep networks are just linear transforms\n",
                "\n",
                "## Sigmoid\n",
                "- Range (0, 1), good for binary classification output\n",
                "- Vanishing gradient problem, avoid in hidden layers\n",
                "\n",
                "## Tanh\n",
                "- Range (-1, 1), zero-centered\n",
                "- Better than sigmoid but still has vanishing gradients\n",
                "\n",
                "## ReLU\n",
                "- Default choice for hidden layers\n",
                "- Fast, no vanishing gradient (for positive values)\n",
                "- Dying ReLU problem\n",
                "\n",
                "## ReLU Variants\n",
                "- Leaky ReLU: Fixes dying ReLU\n",
                "- GELU: Used in transformers\n",
                "- Swish: Self-gated, smooth\n",
                "\n",
                "## Softmax\n",
                "- Converts logits to probabilities\n",
                "- Used for multi-class classification output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Interview Tips\n",
                "\n",
                "---\n",
                "\n",
                "## Common Questions\n",
                "\n",
                "**Q: Why do we need activation functions?**\n",
                "A: Without activation functions, a neural network is just a linear transformation, regardless of depth. Activation functions introduce non-linearity, allowing networks to learn complex patterns.\n",
                "\n",
                "**Q: What is the vanishing gradient problem?**\n",
                "A: When using sigmoid/tanh, gradients get multiplied by values less than 1 during backprop. In deep networks, this causes gradients to shrink exponentially, making early layers learn very slowly.\n",
                "\n",
                "**Q: Why is ReLU preferred over sigmoid in hidden layers?**\n",
                "A: ReLU has gradient of 1 for positive values, avoiding vanishing gradients. It's also computationally efficient. Sigmoid saturates at extremes with near-zero gradients.\n",
                "\n",
                "**Q: What is the dying ReLU problem?**\n",
                "A: If a ReLU neuron's inputs are always negative, it outputs 0 and receives 0 gradient. The neuron stops learning permanently. Solutions include Leaky ReLU or proper initialization.\n",
                "\n",
                "**Q: When would you use sigmoid vs softmax?**\n",
                "A: Sigmoid for binary classification (one probability). Softmax for multi-class classification (probabilities that sum to 1)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Practice Exercises\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Derivative of Sigmoid\n",
                "\n",
                "Prove that $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hint: Use quotient rule on 1/(1+e^(-z))\n",
                "# Your derivation here (in comments)\n",
                "\n",
                "# Numerical verification\n",
                "def numerical_derivative(f, z, h=1e-5):\n",
                "    return (f(z + h) - f(z - h)) / (2 * h)\n",
                "\n",
                "z = 2.0\n",
                "numerical = numerical_derivative(sigmoid, z)\n",
                "analytical = sigmoid(z) * (1 - sigmoid(z))\n",
                "\n",
                "print(f\"Numerical: {numerical:.6f}\")\n",
                "print(f\"Analytical: {analytical:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 2: Implement PReLU\n",
                "\n",
                "Parametric ReLU where the slope for negative values is learned:\n",
                "$$\\text{PReLU}(z) = \\max(0, z) + \\alpha \\min(0, z)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your implementation\n",
                "def prelu(z, alpha):\n",
                "    # Your code here\n",
                "    pass\n",
                "\n",
                "# Test\n",
                "# z = np.array([-2, -1, 0, 1, 2])\n",
                "# print(prelu(z, alpha=0.25))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1 Solution\n",
                "print(\"Exercise 1: Derivative of Sigmoid\")\n",
                "print(\"\"\"\n",
                "sigma(z) = 1 / (1 + e^(-z))\n",
                "\n",
                "Using quotient rule: d/dz [1/u] = -u'/u^2\n",
                "where u = 1 + e^(-z), u' = -e^(-z)\n",
                "\n",
                "sigma'(z) = -(-e^(-z)) / (1 + e^(-z))^2\n",
                "          = e^(-z) / (1 + e^(-z))^2\n",
                "\n",
                "Now, multiply top and bottom by 1/(1+e^(-z)):\n",
                "          = [e^(-z)/(1+e^(-z))] / (1+e^(-z))\n",
                "          = [(1+e^(-z)-1)/(1+e^(-z))] / (1+e^(-z))\n",
                "          = [1 - 1/(1+e^(-z))] * [1/(1+e^(-z))]\n",
                "          = [1 - sigma(z)] * sigma(z)\n",
                "          = sigma(z) * (1 - sigma(z))\n",
                "\"\"\")\n",
                "\n",
                "# Exercise 2 Solution\n",
                "print(\"Exercise 2: PReLU\")\n",
                "def prelu_solution(z, alpha):\n",
                "    return np.maximum(0, z) + alpha * np.minimum(0, z)\n",
                "\n",
                "z = np.array([-2, -1, 0, 1, 2])\n",
                "print(f\"Input: {z}\")\n",
                "print(f\"PReLU(alpha=0.25): {prelu_solution(z, 0.25)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Module: [06 - Perceptron](../06_perceptron/06_perceptron.ipynb)\n",
                "\n",
                "Now that we understand activation functions, let's explore the perceptron - the first learning algorithm for neural networks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}