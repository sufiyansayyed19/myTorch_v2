{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 06: Perceptron\n",
                "\n",
                "**The First Neural Network Learning Algorithm**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "- Understand the perceptron model and algorithm\n",
                "- Implement the perceptron learning algorithm from scratch\n",
                "- Visualize the XOR problem and why single-layer perceptrons fail\n",
                "- Understand how multi-layer perceptrons solve non-linear problems\n",
                "\n",
                "**Prerequisites:** \n",
                "- [Module 04 - The Neuron](../04_the_neuron/04_neuron.ipynb)\n",
                "- [Module 05 - Activation Functions](../05_activation_functions/05_activation_functions.ipynb)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.colors import ListedColormap\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: The Perceptron Model\n",
                "\n",
                "---\n",
                "\n",
                "## 1.1 Historical Context\n",
                "\n",
                "The **perceptron** was invented by Frank Rosenblatt in 1958. It was the first algorithm that could learn from data, and it sparked enormous excitement about AI.\n",
                "\n",
                "The New York Times wrote: \"The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"\n",
                "\n",
                "This excitement was short-lived when Minsky and Papert showed its limitations in 1969, leading to the first \"AI Winter.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 The Perceptron Architecture\n",
                "\n",
                "A perceptron is a single neuron with a **step function** as activation:\n",
                "\n",
                "$$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n",
                "\n",
                "$$y = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$\n",
                "\n",
                "The step function makes it a **binary classifier**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Perceptron\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Step function\n",
                "ax = axes[0]\n",
                "z = np.linspace(-5, 5, 200)\n",
                "step = (z >= 0).astype(float)\n",
                "ax.step(z, step, 'b-', linewidth=2, where='post')\n",
                "ax.scatter([0], [1], color='blue', s=100, zorder=5)\n",
                "ax.scatter([0], [0], color='blue', s=100, facecolors='white', edgecolors='blue', zorder=5)\n",
                "ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "ax.set_xlabel('z')\n",
                "ax.set_ylabel('output')\n",
                "ax.set_title('Step Function (Perceptron Activation)')\n",
                "ax.set_ylim(-0.2, 1.2)\n",
                "\n",
                "# Perceptron diagram\n",
                "ax = axes[1]\n",
                "# Inputs\n",
                "input_y = [0.7, 0.3]\n",
                "for i, y in enumerate(input_y):\n",
                "    ax.scatter([0.15], [y], s=200, c='lightgreen', edgecolors='black', zorder=5)\n",
                "    ax.text(0.05, y, f'$x_{i+1}$', ha='center', va='center', fontsize=12)\n",
                "    ax.annotate('', xy=(0.4, 0.5), xytext=(0.18, y),\n",
                "                arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
                "\n",
                "# Neuron\n",
                "circle = plt.Circle((0.5, 0.5), 0.1, color='lightyellow', ec='black', linewidth=2)\n",
                "ax.add_patch(circle)\n",
                "ax.text(0.5, 0.5, 'Step', ha='center', va='center', fontsize=10)\n",
                "\n",
                "# Output\n",
                "ax.annotate('', xy=(0.8, 0.5), xytext=(0.6, 0.5),\n",
                "            arrowprops=dict(arrowstyle='->', color='red', lw=3))\n",
                "ax.text(0.88, 0.5, 'y\\n(0 or 1)', ha='center', va='center', fontsize=10)\n",
                "\n",
                "ax.set_xlim(0, 1)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.set_aspect('equal')\n",
                "ax.axis('off')\n",
                "ax.set_title('Perceptron Architecture')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: The Perceptron Learning Algorithm\n",
                "\n",
                "---\n",
                "\n",
                "## 2.1 The Algorithm\n",
                "\n",
                "The perceptron learns by updating weights when it makes mistakes:\n",
                "\n",
                "```\n",
                "For each training example (x, y_true):\n",
                "    1. Compute prediction: y_pred = step(w.x + b)\n",
                "    2. If y_pred != y_true:\n",
                "        w = w + learning_rate * (y_true - y_pred) * x\n",
                "        b = b + learning_rate * (y_true - y_pred)\n",
                "```\n",
                "\n",
                "**Intuition:**\n",
                "- If prediction is 0 but should be 1: increase weights (make z larger)\n",
                "- If prediction is 1 but should be 0: decrease weights (make z smaller)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Implementation from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Perceptron:\n",
                "    \"\"\"Perceptron classifier.\"\"\"\n",
                "    \n",
                "    def __init__(self, n_features, learning_rate=0.1):\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0.0\n",
                "        self.lr = learning_rate\n",
                "        self.errors_ = []  # Track errors per epoch\n",
                "    \n",
                "    def step(self, z):\n",
                "        \"\"\"Step activation function.\"\"\"\n",
                "        return np.where(z >= 0, 1, 0)\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
                "        z = np.dot(X, self.weights) + self.bias\n",
                "        return self.step(z)\n",
                "    \n",
                "    def fit(self, X, y, n_epochs=100):\n",
                "        \"\"\"Train the perceptron.\"\"\"\n",
                "        for epoch in range(n_epochs):\n",
                "            errors = 0\n",
                "            for xi, yi in zip(X, y):\n",
                "                # Predict\n",
                "                y_pred = self.predict(xi)\n",
                "                \n",
                "                # Update if wrong\n",
                "                update = self.lr * (yi - y_pred)\n",
                "                self.weights += update * xi\n",
                "                self.bias += update\n",
                "                \n",
                "                # Count errors\n",
                "                errors += int(update != 0)\n",
                "            \n",
                "            self.errors_.append(errors)\n",
                "            \n",
                "            # Converged if no errors\n",
                "            if errors == 0:\n",
                "                print(f\"Converged after {epoch + 1} epochs\")\n",
                "                break\n",
                "        \n",
                "        return self"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Training on Linearly Separable Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate linearly separable data\n",
                "np.random.seed(42)\n",
                "n_samples = 50\n",
                "\n",
                "# Class 0: centered at (1, 1)\n",
                "X0 = np.random.randn(n_samples, 2) * 0.5 + [1, 1]\n",
                "y0 = np.zeros(n_samples)\n",
                "\n",
                "# Class 1: centered at (3, 3)\n",
                "X1 = np.random.randn(n_samples, 2) * 0.5 + [3, 3]\n",
                "y1 = np.ones(n_samples)\n",
                "\n",
                "X = np.vstack([X0, X1])\n",
                "y = np.hstack([y0, y1])\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X0[:, 0], X0[:, 1], color='blue', label='Class 0', edgecolors='k')\n",
                "plt.scatter(X1[:, 0], X1[:, 1], color='red', label='Class 1', edgecolors='k')\n",
                "plt.xlabel('$x_1$')\n",
                "plt.ylabel('$x_2$')\n",
                "plt.title('Linearly Separable Data')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train perceptron\n",
                "perceptron = Perceptron(n_features=2, learning_rate=0.1)\n",
                "perceptron.fit(X, y, n_epochs=100)\n",
                "\n",
                "print(f\"Learned weights: {perceptron.weights}\")\n",
                "print(f\"Learned bias: {perceptron.bias}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot decision boundary\n",
                "def plot_decision_boundary(model, X, y, title):\n",
                "    fig, ax = plt.subplots(figsize=(8, 6))\n",
                "    \n",
                "    # Create mesh grid\n",
                "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
                "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
                "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
                "                         np.linspace(y_min, y_max, 200))\n",
                "    \n",
                "    # Predict on mesh\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "    \n",
                "    # Plot\n",
                "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['blue', 'red']))\n",
                "    ax.scatter(X[y==0, 0], X[y==0, 1], color='blue', label='Class 0', edgecolors='k')\n",
                "    ax.scatter(X[y==1, 0], X[y==1, 1], color='red', label='Class 1', edgecolors='k')\n",
                "    \n",
                "    ax.set_xlabel('$x_1$')\n",
                "    ax.set_ylabel('$x_2$')\n",
                "    ax.set_title(title)\n",
                "    ax.legend()\n",
                "    plt.show()\n",
                "\n",
                "plot_decision_boundary(perceptron, X, y, 'Perceptron Decision Boundary')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot errors per epoch\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(perceptron.errors_, marker='o')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Number of Errors')\n",
                "plt.title('Perceptron Learning: Errors per Epoch')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 The Perceptron Convergence Theorem\n",
                "\n",
                "**Theorem:** If the training data is linearly separable, the perceptron algorithm is guaranteed to converge to a solution (find a separating hyperplane) in a finite number of steps.\n",
                "\n",
                "This was a remarkable result - the first proof that a machine could learn!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: Logic Gates\n",
                "\n",
                "---\n",
                "\n",
                "## 3.1 AND, OR, NOT Gates\n",
                "\n",
                "A single perceptron can learn simple logic gates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# AND gate\n",
                "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_and = np.array([0, 0, 0, 1])\n",
                "\n",
                "perceptron_and = Perceptron(2, learning_rate=0.1)\n",
                "perceptron_and.fit(X_and, y_and, n_epochs=100)\n",
                "\n",
                "print(\"AND Gate:\")\n",
                "for xi, yi in zip(X_and, y_and):\n",
                "    pred = perceptron_and.predict(xi)\n",
                "    print(f\"  {xi} -> {pred} (expected {yi})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OR gate\n",
                "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_or = np.array([0, 1, 1, 1])\n",
                "\n",
                "perceptron_or = Perceptron(2, learning_rate=0.1)\n",
                "perceptron_or.fit(X_or, y_or, n_epochs=100)\n",
                "\n",
                "print(\"OR Gate:\")\n",
                "for xi, yi in zip(X_or, y_or):\n",
                "    pred = perceptron_or.predict(xi)\n",
                "    print(f\"  {xi} -> {pred} (expected {yi})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize AND and OR\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "for ax, (name, X, y, model) in zip(axes, [('AND', X_and, y_and, perceptron_and),\n",
                "                                           ('OR', X_or, y_or, perceptron_or)]):\n",
                "    # Create mesh\n",
                "    xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
                "    \n",
                "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['blue', 'red']))\n",
                "    ax.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=200, edgecolors='k', label='0')\n",
                "    ax.scatter(X[y==1, 0], X[y==1, 1], c='red', s=200, edgecolors='k', label='1')\n",
                "    \n",
                "    ax.set_xlabel('$x_1$')\n",
                "    ax.set_ylabel('$x_2$')\n",
                "    ax.set_title(f'{name} Gate')\n",
                "    ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: The XOR Problem\n",
                "\n",
                "---\n",
                "\n",
                "## 4.1 XOR is Not Linearly Separable\n",
                "\n",
                "The XOR (exclusive or) function:\n",
                "- (0, 0) -> 0\n",
                "- (0, 1) -> 1\n",
                "- (1, 0) -> 1\n",
                "- (1, 1) -> 0\n",
                "\n",
                "There is no single straight line that can separate the two classes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XOR gate\n",
                "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_xor = np.array([0, 1, 1, 0])\n",
                "\n",
                "# Try to train perceptron on XOR\n",
                "perceptron_xor = Perceptron(2, learning_rate=0.1)\n",
                "perceptron_xor.fit(X_xor, y_xor, n_epochs=100)\n",
                "\n",
                "print(\"XOR Gate (Perceptron FAILS):\")\n",
                "for xi, yi in zip(X_xor, y_xor):\n",
                "    pred = perceptron_xor.predict(xi)\n",
                "    status = \"correct\" if pred == yi else \"WRONG\"\n",
                "    print(f\"  {xi} -> {pred} (expected {yi}) {status}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize why XOR fails\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "\n",
                "ax.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=300, edgecolors='k', label='0')\n",
                "ax.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=300, edgecolors='k', label='1')\n",
                "\n",
                "# Annotate points\n",
                "for i, (x, y) in enumerate(X_xor):\n",
                "    ax.annotate(f'({x},{y})->XOR={y_xor[i]}', (x, y), xytext=(5, 5), \n",
                "                textcoords='offset points', fontsize=10)\n",
                "\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('XOR Problem: No Single Line Can Separate Classes\\n(0,0) and (1,1) are class 0; (0,1) and (1,0) are class 1')\n",
                "ax.legend()\n",
                "ax.set_xlim(-0.5, 1.5)\n",
                "ax.set_ylim(-0.5, 1.5)\n",
                "\n",
                "# Show that no line works\n",
                "ax.annotate('No single line\\ncan separate!', xy=(0.5, 0.5), fontsize=12, ha='center',\n",
                "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show perceptron never converges on XOR\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(perceptron_xor.errors_, marker='o')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Number of Errors')\n",
                "plt.title('XOR: Perceptron Never Converges (Always Has Errors)')\n",
                "plt.axhline(y=0, color='r', linestyle='--', label='Perfect (0 errors)')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 Historical Impact\n",
                "\n",
                "In 1969, Marvin Minsky and Seymour Papert published \"Perceptrons\", which mathematically proved that single-layer perceptrons cannot solve XOR or any non-linearly separable problem.\n",
                "\n",
                "This was devastating because many real-world problems are not linearly separable. Funding dried up, and the first \"AI Winter\" began.\n",
                "\n",
                "**The solution was known even then: use multiple layers!** But training multi-layer networks was difficult until backpropagation became popular in the 1980s."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 5: Multi-Layer Perceptron (MLP)\n",
                "\n",
                "---\n",
                "\n",
                "## 5.1 The Solution: Add Hidden Layers\n",
                "\n",
                "By adding a hidden layer, we can learn non-linear decision boundaries.\n",
                "\n",
                "**How XOR can be solved:**\n",
                "\n",
                "XOR(a, b) = OR(AND(a, NOT b), AND(NOT a, b))\n",
                "\n",
                "This requires two hidden neurons!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize MLP architecture for XOR\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Input layer\n",
                "for i, y in enumerate([0.7, 0.3]):\n",
                "    ax.scatter([0.1], [y], s=300, c='lightgreen', edgecolors='black', zorder=5)\n",
                "    ax.text(0.02, y, f'$x_{i+1}$', ha='center', va='center', fontsize=12)\n",
                "\n",
                "# Hidden layer\n",
                "for i, y in enumerate([0.8, 0.5, 0.2]):\n",
                "    ax.scatter([0.4], [y], s=300, c='lightyellow', edgecolors='black', zorder=5)\n",
                "    ax.text(0.4, y, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
                "\n",
                "# Output layer  \n",
                "ax.scatter([0.7], [0.5], s=300, c='salmon', edgecolors='black', zorder=5)\n",
                "ax.text(0.7, 0.5, 'y', ha='center', va='center', fontsize=12)\n",
                "\n",
                "# Connections input to hidden\n",
                "for iy in [0.7, 0.3]:\n",
                "    for hy in [0.8, 0.5, 0.2]:\n",
                "        ax.plot([0.13, 0.37], [iy, hy], 'b-', alpha=0.3, linewidth=1)\n",
                "\n",
                "# Connections hidden to output\n",
                "for hy in [0.8, 0.5, 0.2]:\n",
                "    ax.plot([0.43, 0.67], [hy, 0.5], 'r-', alpha=0.3, linewidth=1)\n",
                "\n",
                "# Labels\n",
                "ax.text(0.1, 0.95, 'Input\\nLayer', ha='center', fontsize=10)\n",
                "ax.text(0.4, 0.95, 'Hidden\\nLayer', ha='center', fontsize=10)\n",
                "ax.text(0.7, 0.95, 'Output\\nLayer', ha='center', fontsize=10)\n",
                "\n",
                "ax.set_xlim(-0.1, 0.85)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.set_aspect('equal')\n",
                "ax.axis('off')\n",
                "ax.set_title('Multi-Layer Perceptron (MLP)\\nHidden layer enables non-linear decision boundaries', fontsize=12)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 Solving XOR with PyTorch MLP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XOR data\n",
                "X_xor_t = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
                "y_xor_t = torch.tensor([[0.], [1.], [1.], [0.]])\n",
                "\n",
                "# MLP with one hidden layer\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(2, 4)   # Hidden layer with 4 neurons\n",
                "        self.fc2 = nn.Linear(4, 1)   # Output layer\n",
                "        self.relu = nn.ReLU()\n",
                "        self.sigmoid = nn.Sigmoid()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu(x)      # Non-linearity!\n",
                "        x = self.fc2(x)\n",
                "        x = self.sigmoid(x)\n",
                "        return x\n",
                "\n",
                "# Train\n",
                "mlp = MLP()\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.1)\n",
                "\n",
                "losses = []\n",
                "for epoch in range(1000):\n",
                "    pred = mlp(X_xor_t)\n",
                "    loss = criterion(pred, y_xor_t)\n",
                "    \n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    losses.append(loss.item())\n",
                "    \n",
                "    if epoch % 200 == 0:\n",
                "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the MLP on XOR\n",
                "print(\"\\nXOR Results with MLP:\")\n",
                "with torch.no_grad():\n",
                "    predictions = mlp(X_xor_t)\n",
                "    for xi, yi, pi in zip(X_xor_t, y_xor_t, predictions):\n",
                "        pred_class = 1 if pi.item() > 0.5 else 0\n",
                "        print(f\"  {xi.tolist()} -> {pi.item():.3f} (class {pred_class}, expected {int(yi.item())})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize MLP decision boundary for XOR\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Training loss\n",
                "axes[0].plot(losses)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('MLP Training on XOR')\n",
                "axes[0].set_yscale('log')\n",
                "\n",
                "# Decision boundary\n",
                "ax = axes[1]\n",
                "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
                "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
                "\n",
                "with torch.no_grad():\n",
                "    Z = mlp(grid).numpy().reshape(xx.shape)\n",
                "\n",
                "ax.contourf(xx, yy, Z, levels=50, cmap='RdBu_r', alpha=0.8)\n",
                "ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
                "ax.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=200, edgecolors='k', label='0')\n",
                "ax.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=200, edgecolors='k', label='1')\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('MLP Solves XOR!\\n(Non-linear decision boundary)')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.3 How Does the Hidden Layer Help?\n",
                "\n",
                "The hidden layer transforms the input space into a new space where the problem becomes linearly separable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the hidden layer representation\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Original space\n",
                "ax = axes[0]\n",
                "ax.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=200, edgecolors='k', label='0')\n",
                "ax.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=200, edgecolors='k', label='1')\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('Original Space\\n(Not linearly separable)')\n",
                "ax.legend()\n",
                "\n",
                "# Hidden space\n",
                "ax = axes[1]\n",
                "with torch.no_grad():\n",
                "    hidden = torch.relu(mlp.fc1(X_xor_t))  # Get hidden layer outputs\n",
                "\n",
                "# Use first two hidden dimensions for visualization\n",
                "h = hidden.numpy()\n",
                "ax.scatter(h[y_xor==0, 0], h[y_xor==0, 1], c='blue', s=200, edgecolors='k', label='0')\n",
                "ax.scatter(h[y_xor==1, 0], h[y_xor==1, 1], c='red', s=200, edgecolors='k', label='1')\n",
                "ax.set_xlabel('$h_1$')\n",
                "ax.set_ylabel('$h_2$')\n",
                "ax.set_title('Hidden Layer Space\\n(Linearly separable!)')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Key Points Summary\n",
                "\n",
                "---\n",
                "\n",
                "## The Perceptron\n",
                "- Single neuron with step activation\n",
                "- Learns by updating weights when wrong\n",
                "- Guaranteed to converge for linearly separable data\n",
                "\n",
                "## Limitations\n",
                "- Can only learn linearly separable patterns\n",
                "- Cannot solve XOR (proved by Minsky & Papert)\n",
                "- This limitation led to the first AI Winter\n",
                "\n",
                "## Multi-Layer Perceptron\n",
                "- Adding hidden layers enables non-linear boundaries\n",
                "- Hidden layers transform input to a separable space\n",
                "- Foundation of modern deep learning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Interview Tips\n",
                "\n",
                "---\n",
                "\n",
                "## Common Questions\n",
                "\n",
                "**Q: What is a perceptron?**\n",
                "A: A perceptron is a single-layer neural network with a step activation function. It computes a weighted sum of inputs, adds a bias, and outputs 1 if the result is positive, 0 otherwise.\n",
                "\n",
                "**Q: What is the XOR problem?**\n",
                "A: XOR is a classic example of a non-linearly separable problem that single-layer perceptrons cannot solve. The inputs (0,0) and (1,1) map to 0, while (0,1) and (1,0) map to 1. No single straight line can separate these classes.\n",
                "\n",
                "**Q: How do you solve the XOR problem?**\n",
                "A: By using a multi-layer perceptron (MLP) with at least one hidden layer. The hidden layer transforms the input space into a new representation where the classes become linearly separable.\n",
                "\n",
                "**Q: What is the perceptron convergence theorem?**\n",
                "A: It states that if the training data is linearly separable, the perceptron learning algorithm will converge to a solution in a finite number of steps."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Practice Exercises\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Implement NAND Gate\n",
                "\n",
                "Train a perceptron to learn NAND (NOT AND):\n",
                "- (0,0) -> 1\n",
                "- (0,1) -> 1  \n",
                "- (1,0) -> 1\n",
                "- (1,1) -> 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "X_nand = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_nand = np.array([1, 1, 1, 0])\n",
                "\n",
                "# Train perceptron"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 2: Visualize Decision Boundary Animation\n",
                "\n",
                "Modify the Perceptron class to store weights at each epoch and visualize how the decision boundary evolves during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1 Solution\n",
                "print(\"Exercise 1: NAND Gate\")\n",
                "X_nand = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_nand = np.array([1, 1, 1, 0])\n",
                "\n",
                "perceptron_nand = Perceptron(2, learning_rate=0.1)\n",
                "perceptron_nand.fit(X_nand, y_nand, n_epochs=100)\n",
                "\n",
                "print(\"NAND Gate:\")\n",
                "for xi, yi in zip(X_nand, y_nand):\n",
                "    pred = perceptron_nand.predict(xi)\n",
                "    print(f\"  {xi} -> {pred} (expected {yi})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Module: [07 - Loss Functions](../07_loss_functions/07_loss_functions.ipynb)\n",
                "\n",
                "Now that we understand the limitations of simple perceptrons and how MLPs can overcome them, let's dive into loss functions - how we measure model error."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}