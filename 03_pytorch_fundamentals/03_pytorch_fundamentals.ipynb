{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 03: PyTorch Fundamentals\n",
                "\n",
                "**The Foundation of Deep Learning in PyTorch**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "- Master PyTorch tensor creation and operations\n",
                "- Understand the relationship between NumPy and PyTorch\n",
                "- Know how to use GPU acceleration\n",
                "- Understand automatic differentiation (autograd)\n",
                "\n",
                "**Prerequisites:** \n",
                "- [Module 01 - Python & Math Prerequisites](../01_python_math_prerequisites/01_prerequisites.ipynb)\n",
                "- [Module 02 - Introduction to Deep Learning](../02_intro_to_deep_learning/02_intro_deep_learning.ipynb)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: Tensors - The Core Data Structure\n",
                "\n",
                "---\n",
                "\n",
                "## 1.1 What is a Tensor?\n",
                "\n",
                "A **tensor** is a multi-dimensional array. It's the fundamental data structure in PyTorch.\n",
                "\n",
                "| Dimensions | Name | Example |\n",
                "|------------|------|--------|\n",
                "| 0 | Scalar | Single number: `5` |\n",
                "| 1 | Vector | List: `[1, 2, 3]` |\n",
                "| 2 | Matrix | 2D array: image (height x width) |\n",
                "| 3 | 3D Tensor | Batch of images, video frame |\n",
                "| 4+ | N-D Tensor | Batch of videos, etc. |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Creating Tensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# From Python list\n",
                "t1 = torch.tensor([1, 2, 3])\n",
                "print(f\"From list: {t1}\")\n",
                "\n",
                "# 2D tensor\n",
                "t2 = torch.tensor([[1, 2, 3],\n",
                "                   [4, 5, 6]])\n",
                "print(f\"\\n2D tensor:\\n{t2}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Common initialization patterns\n",
                "zeros = torch.zeros(3, 4)       # 3x4 zeros\n",
                "ones = torch.ones(2, 3)         # 2x3 ones  \n",
                "eye = torch.eye(3)              # 3x3 identity\n",
                "empty = torch.empty(2, 2)       # Uninitialized (random memory)\n",
                "\n",
                "print(f\"Zeros (3x4):\\n{zeros}\")\n",
                "print(f\"\\nIdentity (3x3):\\n{eye}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random tensors - essential for weight initialization\n",
                "rand_uniform = torch.rand(2, 3)      # Uniform [0, 1)\n",
                "rand_normal = torch.randn(2, 3)      # Standard normal N(0, 1)\n",
                "rand_int = torch.randint(0, 10, (2, 3))  # Random integers [0, 10)\n",
                "\n",
                "print(f\"Uniform [0,1):\\n{rand_uniform}\")\n",
                "print(f\"\\nNormal (0,1):\\n{rand_normal}\")\n",
                "print(f\"\\nRandom integers [0,10):\\n{rand_int}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Range and linspace\n",
                "arange = torch.arange(0, 10, 2)      # Start, end, step\n",
                "linspace = torch.linspace(0, 1, 5)   # Start, end, num_points\n",
                "\n",
                "print(f\"Arange (0 to 10 step 2): {arange}\")\n",
                "print(f\"Linspace (0 to 1, 5 points): {linspace}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create tensor with same properties as another\n",
                "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
                "\n",
                "zeros_like = torch.zeros_like(x)     # Same shape, dtype, device\n",
                "ones_like = torch.ones_like(x)\n",
                "rand_like = torch.randn_like(x)\n",
                "\n",
                "print(f\"Original:\\n{x}\")\n",
                "print(f\"\\nZeros like:\\n{zeros_like}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Tensor Attributes\n",
                "\n",
                "Every tensor has three key attributes:\n",
                "- **shape**: Dimensions of the tensor\n",
                "- **dtype**: Data type of elements\n",
                "- **device**: Where the tensor lives (CPU or GPU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.randn(3, 4)\n",
                "\n",
                "print(f\"Tensor:\\n{x}\")\n",
                "print(f\"\\nShape: {x.shape}\")        # or x.size()\n",
                "print(f\"Dtype: {x.dtype}\")\n",
                "print(f\"Device: {x.device}\")\n",
                "print(f\"Number of dimensions: {x.ndim}\")\n",
                "print(f\"Total elements: {x.numel()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data types\n",
                "# float32 is the default and most common for deep learning\n",
                "float_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)\n",
                "double_tensor = torch.tensor([1.0, 2.0], dtype=torch.float64)\n",
                "int_tensor = torch.tensor([1, 2], dtype=torch.int64)\n",
                "bool_tensor = torch.tensor([True, False], dtype=torch.bool)\n",
                "\n",
                "print(f\"float32: {float_tensor.dtype}\")\n",
                "print(f\"float64: {double_tensor.dtype}\")\n",
                "print(f\"int64: {int_tensor.dtype}\")\n",
                "print(f\"bool: {bool_tensor.dtype}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Why float32?\n",
                "\n",
                "- **Memory efficient**: Half the memory of float64\n",
                "- **Faster on GPU**: GPUs are optimized for float32\n",
                "- **Sufficient precision**: Neural networks don't need float64 precision\n",
                "\n",
                "For training very large models, even float16 (half precision) is used!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Reshaping Tensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.arange(12)\n",
                "print(f\"Original: {x}\")\n",
                "print(f\"Shape: {x.shape}\")\n",
                "\n",
                "# Reshape\n",
                "reshaped = x.reshape(3, 4)\n",
                "print(f\"\\nReshaped (3, 4):\\n{reshaped}\")\n",
                "\n",
                "# View (requires contiguous memory, but no copy)\n",
                "viewed = x.view(4, 3)\n",
                "print(f\"\\nViewed (4, 3):\\n{viewed}\")\n",
                "\n",
                "# -1 means \"infer this dimension\"\n",
                "auto_reshape = x.reshape(2, -1)  # 2 rows, auto columns\n",
                "print(f\"\\nAuto reshape (2, -1) = (2, 6):\\n{auto_reshape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Adding/removing dimensions\n",
                "x = torch.randn(3, 4)\n",
                "print(f\"Original shape: {x.shape}\")\n",
                "\n",
                "# Unsqueeze: add dimension\n",
                "x_unsqueeze = x.unsqueeze(0)  # Add at position 0\n",
                "print(f\"After unsqueeze(0): {x_unsqueeze.shape}\")\n",
                "\n",
                "x_unsqueeze2 = x.unsqueeze(-1)  # Add at last position\n",
                "print(f\"After unsqueeze(-1): {x_unsqueeze2.shape}\")\n",
                "\n",
                "# Squeeze: remove dimensions of size 1\n",
                "y = torch.randn(1, 3, 1, 4)\n",
                "print(f\"\\nBefore squeeze: {y.shape}\")\n",
                "print(f\"After squeeze: {y.squeeze().shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flatten\n",
                "x = torch.randn(2, 3, 4)\n",
                "print(f\"Original shape: {x.shape}\")\n",
                "\n",
                "flat = x.flatten()  # Completely flat\n",
                "print(f\"Fully flattened: {flat.shape}\")\n",
                "\n",
                "# Flatten starting from dimension 1 (keep batch)\n",
                "batch_flat = x.flatten(start_dim=1)\n",
                "print(f\"Flatten from dim 1: {batch_flat.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Deep Learning Context\n",
                "\n",
                "Common reshape patterns:\n",
                "- **Flatten before dense layer**: (batch, C, H, W) -> (batch, C*H*W)\n",
                "- **Add batch dimension**: (features,) -> (1, features)\n",
                "- **Prepare for broadcasting**: (batch, features) -> (batch, features, 1, 1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5 Tensor Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Element-wise operations\n",
                "a = torch.tensor([1.0, 2.0, 3.0])\n",
                "b = torch.tensor([4.0, 5.0, 6.0])\n",
                "\n",
                "print(f\"a = {a}\")\n",
                "print(f\"b = {b}\")\n",
                "print(f\"\\na + b = {a + b}\")\n",
                "print(f\"a * b = {a * b}\")\n",
                "print(f\"a / b = {a / b}\")\n",
                "print(f\"a ** 2 = {a ** 2}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mathematical functions\n",
                "x = torch.tensor([0.0, 1.0, 2.0])\n",
                "\n",
                "print(f\"x = {x}\")\n",
                "print(f\"exp(x) = {torch.exp(x)}\")\n",
                "print(f\"log(exp(x)) = {torch.log(torch.exp(x))}\")\n",
                "print(f\"sqrt(x+1) = {torch.sqrt(x + 1)}\")\n",
                "print(f\"sin(x) = {torch.sin(x)}\")\n",
                "print(f\"abs(x-1) = {torch.abs(x - 1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reduction operations\n",
                "x = torch.tensor([[1.0, 2.0, 3.0],\n",
                "                  [4.0, 5.0, 6.0]])\n",
                "\n",
                "print(f\"Tensor:\\n{x}\")\n",
                "print(f\"\\nSum (all): {x.sum()}\")\n",
                "print(f\"Sum (dim=0, columns): {x.sum(dim=0)}\")\n",
                "print(f\"Sum (dim=1, rows): {x.sum(dim=1)}\")\n",
                "print(f\"Mean: {x.mean()}\")\n",
                "print(f\"Std: {x.std()}\")\n",
                "print(f\"Max: {x.max()}\")\n",
                "print(f\"Argmax: {x.argmax()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Matrix operations\n",
                "A = torch.randn(3, 4)\n",
                "B = torch.randn(4, 2)\n",
                "\n",
                "# Matrix multiplication\n",
                "C = A @ B  # or torch.matmul(A, B)\n",
                "print(f\"A shape: {A.shape}\")\n",
                "print(f\"B shape: {B.shape}\")\n",
                "print(f\"A @ B shape: {C.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dot product (1D vectors)\n",
                "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
                "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
                "\n",
                "dot = torch.dot(v1, v2)\n",
                "print(f\"v1 . v2 = {dot}\")  # 1*4 + 2*5 + 3*6 = 32"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.6 Indexing and Slicing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.arange(12).reshape(3, 4)\n",
                "print(f\"Tensor:\\n{x}\")\n",
                "\n",
                "# Basic indexing\n",
                "print(f\"\\nx[0, 0] = {x[0, 0]}\")\n",
                "print(f\"x[1, 2] = {x[1, 2]}\")\n",
                "print(f\"x[-1] (last row) = {x[-1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Slicing\n",
                "print(f\"First two rows:\\n{x[:2]}\")\n",
                "print(f\"\\nColumns 1 to 3:\\n{x[:, 1:3]}\")\n",
                "print(f\"\\nEvery other row:\\n{x[::2]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Boolean indexing\n",
                "x = torch.randn(5)\n",
                "print(f\"x = {x}\")\n",
                "print(f\"x > 0: {x > 0}\")\n",
                "print(f\"x[x > 0] = {x[x > 0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.7 In-place Operations\n",
                "\n",
                "Operations with `_` suffix modify the tensor in-place."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor([1.0, 2.0, 3.0])\n",
                "print(f\"Original: {x}\")\n",
                "\n",
                "# In-place operations (modify x directly)\n",
                "x.add_(10)  # x = x + 10\n",
                "print(f\"After add_(10): {x}\")\n",
                "\n",
                "x.mul_(2)   # x = x * 2\n",
                "print(f\"After mul_(2): {x}\")\n",
                "\n",
                "x.zero_()   # x = 0\n",
                "print(f\"After zero_(): {x}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Warning!\n",
                "\n",
                "In-place operations can cause problems with autograd. Avoid them when tracking gradients."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: PyTorch and NumPy\n",
                "\n",
                "---\n",
                "\n",
                "## 2.1 Conversion Between NumPy and PyTorch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NumPy to PyTorch\n",
                "np_array = np.array([1, 2, 3, 4, 5])\n",
                "print(f\"NumPy array: {np_array}\")\n",
                "\n",
                "# Method 1: from_numpy (shares memory!)\n",
                "tensor_shared = torch.from_numpy(np_array)\n",
                "print(f\"Tensor (shared): {tensor_shared}\")\n",
                "\n",
                "# Method 2: torch.tensor (copies data)\n",
                "tensor_copied = torch.tensor(np_array)\n",
                "print(f\"Tensor (copied): {tensor_copied}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory sharing demonstration\n",
                "np_arr = np.array([1, 2, 3])\n",
                "t_shared = torch.from_numpy(np_arr)\n",
                "\n",
                "print(f\"Before: NumPy = {np_arr}, Tensor = {t_shared}\")\n",
                "\n",
                "# Modify NumPy array\n",
                "np_arr[0] = 999\n",
                "\n",
                "print(f\"After modifying NumPy: NumPy = {np_arr}, Tensor = {t_shared}\")\n",
                "print(\"The tensor changed too! They share memory.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch to NumPy\n",
                "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
                "\n",
                "# .numpy() shares memory (CPU tensor only)\n",
                "np_from_tensor = tensor.numpy()\n",
                "print(f\"Tensor to NumPy: {np_from_tensor}\")\n",
                "print(f\"Type: {type(np_from_tensor)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Key Differences\n",
                "\n",
                "| Feature | NumPy | PyTorch |\n",
                "|---------|-------|--------|\n",
                "| GPU support | No | Yes |\n",
                "| Automatic differentiation | No | Yes |\n",
                "| Deep learning layers | No | Yes |\n",
                "| Naming | ndarray | tensor |\n",
                "| Primary use | Numerical computing | Deep learning |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: GPU Acceleration\n",
                "\n",
                "---\n",
                "\n",
                "## 3.1 Device Management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
                "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
                "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Device-agnostic code pattern\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Moving tensors to device\n",
                "x = torch.randn(3, 4)\n",
                "print(f\"Original device: {x.device}\")\n",
                "\n",
                "# Move to device (GPU if available, else stays on CPU)\n",
                "x = x.to(device)\n",
                "print(f\"After .to(device): {x.device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create tensor directly on device\n",
                "y = torch.randn(3, 4, device=device)\n",
                "print(f\"Created on device: {y.device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Moving back to CPU (needed for NumPy, plotting, etc.)\n",
                "x_cpu = x.cpu()\n",
                "print(f\"After .cpu(): {x_cpu.device}\")\n",
                "\n",
                "# Convert to NumPy (must be on CPU)\n",
                "x_numpy = x_cpu.numpy()\n",
                "print(f\"As NumPy array: {x_numpy}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Operations Across Devices\n",
                "\n",
                "**Important:** All tensors in an operation must be on the same device!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This would cause an error if one tensor is on GPU and one on CPU\n",
                "# a_cpu = torch.randn(3)\n",
                "# b_gpu = torch.randn(3, device='cuda')\n",
                "# c = a_cpu + b_gpu  # ERROR!\n",
                "\n",
                "# Correct approach: move to same device\n",
                "a = torch.randn(3, device=device)\n",
                "b = torch.randn(3, device=device)\n",
                "c = a + b  # Works!\n",
                "print(f\"Result device: {c.device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 GPU Speed Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "# Matrix multiplication benchmark\n",
                "sizes = [1000, 2000, 4000]\n",
                "\n",
                "for size in sizes:\n",
                "    # CPU\n",
                "    a_cpu = torch.randn(size, size)\n",
                "    b_cpu = torch.randn(size, size)\n",
                "    \n",
                "    start = time.time()\n",
                "    c_cpu = a_cpu @ b_cpu\n",
                "    cpu_time = time.time() - start\n",
                "    \n",
                "    # GPU (if available)\n",
                "    if torch.cuda.is_available():\n",
                "        a_gpu = a_cpu.cuda()\n",
                "        b_gpu = b_cpu.cuda()\n",
                "        \n",
                "        torch.cuda.synchronize()  # Wait for GPU operations\n",
                "        start = time.time()\n",
                "        c_gpu = a_gpu @ b_gpu\n",
                "        torch.cuda.synchronize()\n",
                "        gpu_time = time.time() - start\n",
                "        \n",
                "        speedup = cpu_time / gpu_time\n",
                "        print(f\"Size {size}x{size}: CPU={cpu_time:.4f}s, GPU={gpu_time:.4f}s, Speedup={speedup:.1f}x\")\n",
                "    else:\n",
                "        print(f\"Size {size}x{size}: CPU={cpu_time:.4f}s (GPU not available)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: Automatic Differentiation (Autograd)\n",
                "\n",
                "---\n",
                "\n",
                "This is one of PyTorch's most powerful features. It automatically computes gradients, which is essential for training neural networks.\n",
                "\n",
                "## 4.1 The Concept\n",
                "\n",
                "Recall from calculus: to minimize a loss function, we need its gradient.\n",
                "\n",
                "**Manually computing gradients is:**\n",
                "- Tedious for complex networks\n",
                "- Error-prone\n",
                "- Hard to modify\n",
                "\n",
                "**Autograd:**\n",
                "- Tracks operations on tensors\n",
                "- Automatically computes gradients\n",
                "- Handles arbitrarily complex graphs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 requires_grad"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# By default, tensors don't track gradients\n",
                "x = torch.tensor([1.0, 2.0, 3.0])\n",
                "print(f\"requires_grad: {x.requires_grad}\")\n",
                "\n",
                "# Enable gradient tracking\n",
                "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
                "print(f\"requires_grad: {x.requires_grad}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# When requires_grad=True, operations build a computation graph\n",
                "x = torch.tensor([2.0], requires_grad=True)\n",
                "y = x ** 2  # y = x^2\n",
                "z = 2 * y   # z = 2 * x^2\n",
                "\n",
                "print(f\"x = {x}\")\n",
                "print(f\"y = x^2 = {y}\")\n",
                "print(f\"z = 2*y = {z}\")\n",
                "print(f\"\\ny.grad_fn: {y.grad_fn}\")  # Tracks how y was created\n",
                "print(f\"z.grad_fn: {z.grad_fn}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.3 Computing Gradients with backward()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple example: y = x^2, find dy/dx\n",
                "x = torch.tensor([2.0], requires_grad=True)\n",
                "y = x ** 2\n",
                "\n",
                "# Compute gradients\n",
                "y.backward()  # dy/dx\n",
                "\n",
                "print(f\"x = {x.item()}\")\n",
                "print(f\"y = x^2 = {y.item()}\")\n",
                "print(f\"dy/dx = 2x = {x.grad.item()}\")  # 2 * 2 = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# More complex example: chain rule\n",
                "# y = (x + 2)^2\n",
                "# dy/dx = 2(x + 2)\n",
                "\n",
                "x = torch.tensor([3.0], requires_grad=True)\n",
                "y = (x + 2) ** 2\n",
                "\n",
                "y.backward()\n",
                "\n",
                "print(f\"x = {x.item()}\")\n",
                "print(f\"y = (x+2)^2 = {y.item()}\")\n",
                "print(f\"dy/dx = 2(x+2) = {x.grad.item()}\")  # 2 * (3 + 2) = 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multiple variables\n",
                "# z = x^2 + y^2\n",
                "# dz/dx = 2x, dz/dy = 2y\n",
                "\n",
                "x = torch.tensor([3.0], requires_grad=True)\n",
                "y = torch.tensor([4.0], requires_grad=True)\n",
                "\n",
                "z = x**2 + y**2\n",
                "\n",
                "z.backward()\n",
                "\n",
                "print(f\"z = x^2 + y^2 = {z.item()}\")\n",
                "print(f\"dz/dx = 2x = {x.grad.item()}\")  # 6\n",
                "print(f\"dz/dy = 2y = {y.grad.item()}\")  # 8"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.4 Vector-Valued Functions\n",
                "\n",
                "When the output is a vector, we need to provide a gradient argument."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For neural networks, we usually sum the loss first\n",
                "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
                "y = x ** 2\n",
                "loss = y.sum()  # Scalar output\n",
                "\n",
                "loss.backward()\n",
                "\n",
                "print(f\"x = {x}\")\n",
                "print(f\"y = x^2 = {y}\")\n",
                "print(f\"loss = sum(y) = {loss.item()}\")\n",
                "print(f\"d(loss)/dx = 2x = {x.grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.5 Gradient Accumulation\n",
                "\n",
                "**Important:** Gradients accumulate by default! You must zero them before each backward pass."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.tensor([2.0], requires_grad=True)\n",
                "\n",
                "# First backward\n",
                "y = x ** 2\n",
                "y.backward()\n",
                "print(f\"After first backward: grad = {x.grad.item()}\")\n",
                "\n",
                "# Second backward (without zeroing)\n",
                "y = x ** 2\n",
                "y.backward()\n",
                "print(f\"After second backward (accumulated!): grad = {x.grad.item()}\")\n",
                "\n",
                "# Correct way: zero gradients first\n",
                "x.grad.zero_()\n",
                "y = x ** 2\n",
                "y.backward()\n",
                "print(f\"After zeroing and backward: grad = {x.grad.item()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.6 Detaching and No-Grad Context"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detach: remove from computation graph\n",
                "x = torch.tensor([2.0], requires_grad=True)\n",
                "y = x ** 2\n",
                "\n",
                "y_detached = y.detach()  # No longer tracks gradients\n",
                "print(f\"y requires_grad: {y.requires_grad}\")\n",
                "print(f\"y_detached requires_grad: {y_detached.requires_grad}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# torch.no_grad(): disable gradient tracking temporarily\n",
                "# Used during inference/evaluation\n",
                "\n",
                "x = torch.tensor([2.0], requires_grad=True)\n",
                "\n",
                "# Normal operation (tracks gradients)\n",
                "y = x ** 2\n",
                "print(f\"Normal: y.requires_grad = {y.requires_grad}\")\n",
                "\n",
                "# With no_grad\n",
                "with torch.no_grad():\n",
                "    y_no_grad = x ** 2\n",
                "    print(f\"In no_grad: y.requires_grad = {y_no_grad.requires_grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### When to Use No-Grad\n",
                "\n",
                "- **Inference/Evaluation**: No need to compute gradients\n",
                "- **Memory saving**: Gradient tracking uses memory\n",
                "- **Speed**: Slightly faster without tracking"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.7 A Neural Network Gradient Example\n",
                "\n",
                "Let's see how autograd works for a simple one-layer network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple neuron: y = sigmoid(w*x + b)\n",
                "# Loss = (y - target)^2\n",
                "\n",
                "# Input\n",
                "x = torch.tensor([1.0])\n",
                "target = torch.tensor([0.0])\n",
                "\n",
                "# Learnable parameters\n",
                "w = torch.tensor([0.5], requires_grad=True)\n",
                "b = torch.tensor([0.1], requires_grad=True)\n",
                "\n",
                "# Forward pass\n",
                "z = w * x + b\n",
                "y = torch.sigmoid(z)\n",
                "loss = (y - target) ** 2\n",
                "\n",
                "print(f\"w = {w.item():.3f}, b = {b.item():.3f}\")\n",
                "print(f\"z = w*x + b = {z.item():.3f}\")\n",
                "print(f\"y = sigmoid(z) = {y.item():.3f}\")\n",
                "print(f\"loss = (y - target)^2 = {loss.item():.3f}\")\n",
                "\n",
                "# Backward pass\n",
                "loss.backward()\n",
                "\n",
                "print(f\"\\nGradients:\")\n",
                "print(f\"d(loss)/dw = {w.grad.item():.4f}\")\n",
                "print(f\"d(loss)/db = {b.grad.item():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's verify the gradient manually using chain rule\n",
                "# loss = (sigmoid(w*x + b) - target)^2\n",
                "# d(loss)/dw = 2*(y-target) * y*(1-y) * x\n",
                "\n",
                "y_val = y.item()\n",
                "manual_grad_w = 2 * (y_val - target.item()) * y_val * (1 - y_val) * x.item()\n",
                "print(f\"Manual d(loss)/dw = {manual_grad_w:.4f}\")\n",
                "print(f\"Autograd d(loss)/dw = {w.grad.item():.4f}\")\n",
                "print(f\"Match: {abs(manual_grad_w - w.grad.item()) < 1e-6}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Key Points Summary\n",
                "\n",
                "---\n",
                "\n",
                "## Tensors\n",
                "- Tensors are multi-dimensional arrays, the core data structure\n",
                "- Key attributes: shape, dtype, device\n",
                "- Use float32 for deep learning (memory efficient, fast on GPU)\n",
                "\n",
                "## NumPy Integration\n",
                "- `torch.from_numpy()` shares memory\n",
                "- `torch.tensor()` copies data\n",
                "- `.numpy()` converts to NumPy (CPU only)\n",
                "\n",
                "## GPU Acceleration\n",
                "- Use `device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')`\n",
                "- Move tensors with `.to(device)`\n",
                "- All tensors in an operation must be on the same device\n",
                "\n",
                "## Autograd\n",
                "- Set `requires_grad=True` to track gradients\n",
                "- Call `.backward()` to compute gradients\n",
                "- Gradients accumulate - zero them with `.zero_()`\n",
                "- Use `torch.no_grad()` during inference"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Interview Tips\n",
                "\n",
                "---\n",
                "\n",
                "## Common Questions\n",
                "\n",
                "**Q: What is a tensor?**\n",
                "A: A tensor is a multi-dimensional array, similar to a NumPy ndarray but with GPU support and automatic differentiation. It's the fundamental data structure in PyTorch.\n",
                "\n",
                "**Q: What is the difference between `.view()` and `.reshape()`?**\n",
                "A: `.view()` requires the tensor to be contiguous in memory and returns a view (shared memory). `.reshape()` works on any tensor and may return a copy if needed. Use `.reshape()` when unsure.\n",
                "\n",
                "**Q: How does autograd work?**\n",
                "A: When `requires_grad=True`, PyTorch builds a computational graph that records operations. When `.backward()` is called, it traverses this graph backwards, applying the chain rule to compute gradients.\n",
                "\n",
                "**Q: Why do we need to zero gradients?**\n",
                "A: PyTorch accumulates gradients by default. This is useful for implementing gradient accumulation across batches, but during normal training, we need to zero gradients before each backward pass.\n",
                "\n",
                "**Q: What is the purpose of `torch.no_grad()`?**\n",
                "A: It temporarily disables gradient computation. Used during inference to save memory and speed up computation since we don't need gradients for predictions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Practice Exercises\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Tensor Creation\n",
                "\n",
                "Create a 3x4 tensor with values from a normal distribution with mean=5 and std=2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "tensor = None  # Replace\n",
                "\n",
                "# Verify\n",
                "# print(f\"Shape: {tensor.shape}\")\n",
                "# print(f\"Approximate mean: {tensor.mean():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 2: Reshaping\n",
                "\n",
                "Given a batch of 32 images of shape (32, 3, 28, 28), flatten each image to feed into a linear layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "images = torch.randn(32, 3, 28, 28)\n",
                "\n",
                "# Your code here\n",
                "# Reshape to (32, 3*28*28) = (32, 2352)\n",
                "flattened = None  # Replace\n",
                "\n",
                "# print(f\"Flattened shape: {flattened.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 3: Gradient Computation\n",
                "\n",
                "For f(x) = x^3 - 2x^2 + x, compute df/dx at x=3 using autograd."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "x = None  # Create tensor with requires_grad=True\n",
                "# Compute f and backward\n",
                "\n",
                "# The analytical derivative is: 3x^2 - 4x + 1\n",
                "# At x=3: 3*9 - 12 + 1 = 16"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1\n",
                "print(\"Exercise 1:\")\n",
                "tensor = torch.randn(3, 4) * 2 + 5  # N(0,1) * std + mean\n",
                "print(f\"Shape: {tensor.shape}\")\n",
                "print(f\"Approximate mean: {tensor.mean():.2f}\")\n",
                "\n",
                "# Exercise 2\n",
                "print(\"\\nExercise 2:\")\n",
                "images = torch.randn(32, 3, 28, 28)\n",
                "flattened = images.flatten(start_dim=1)  # or images.view(32, -1)\n",
                "print(f\"Flattened shape: {flattened.shape}\")\n",
                "\n",
                "# Exercise 3\n",
                "print(\"\\nExercise 3:\")\n",
                "x = torch.tensor([3.0], requires_grad=True)\n",
                "f = x**3 - 2*x**2 + x\n",
                "f.backward()\n",
                "print(f\"f(3) = {f.item()}\")\n",
                "print(f\"df/dx at x=3: {x.grad.item()} (analytical: 16)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Module: [04 - The Neuron](../04_the_neuron/04_neuron.ipynb)\n",
                "\n",
                "Now that we understand PyTorch basics, let's dive into the fundamental building block of neural networks - the artificial neuron."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}