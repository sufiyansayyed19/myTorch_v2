{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 04: The Neuron - Building Block of Neural Networks\n",
                "\n",
                "**Understanding the Fundamental Unit**\n",
                "\n",
                "---\n",
                "\n",
                "## Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "- Understand the biological inspiration for artificial neurons\n",
                "- Master the mathematical model of a neuron\n",
                "- Implement a neuron from scratch in NumPy and PyTorch\n",
                "- Understand how neurons process information\n",
                "\n",
                "**Prerequisites:** \n",
                "- [Module 01 - Python & Math Prerequisites](../01_python_math_prerequisites/01_prerequisites.ipynb)\n",
                "- [Module 03 - PyTorch Fundamentals](../03_pytorch_fundamentals/03_pytorch_fundamentals.ipynb)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 1: Biological Inspiration\n",
                "\n",
                "---\n",
                "\n",
                "## 1.1 The Biological Neuron\n",
                "\n",
                "The artificial neuron is inspired by biological neurons in our brains.\n",
                "\n",
                "### Structure of a Biological Neuron:\n",
                "\n",
                "1. **Dendrites**: Receive signals from other neurons\n",
                "2. **Cell Body (Soma)**: Processes the incoming signals\n",
                "3. **Axon**: Transmits the output signal\n",
                "4. **Synapses**: Connections to other neurons\n",
                "\n",
                "### How It Works:\n",
                "\n",
                "1. Dendrites receive electrical signals from other neurons\n",
                "2. Signals are accumulated in the cell body\n",
                "3. If the accumulated signal exceeds a threshold, the neuron \"fires\"\n",
                "4. The signal travels down the axon to other neurons"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Biological Neuron\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Cell body (soma)\n",
                "soma = plt.Circle((0.4, 0.5), 0.1, color='lightblue', ec='black', linewidth=2)\n",
                "ax.add_patch(soma)\n",
                "ax.text(0.4, 0.5, 'Soma\\n(Cell Body)', ha='center', va='center', fontsize=9)\n",
                "\n",
                "# Dendrites (inputs)\n",
                "dendrite_ends = [(0.05, 0.8), (0.05, 0.6), (0.05, 0.4), (0.05, 0.2)]\n",
                "for i, (x, y) in enumerate(dendrite_ends):\n",
                "    ax.annotate('', xy=(0.3, 0.5), xytext=(x, y),\n",
                "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
                "    ax.text(x-0.02, y, f'Input {i+1}', ha='right', va='center', fontsize=8)\n",
                "\n",
                "ax.text(0.15, 0.9, 'Dendrites\\n(Receive signals)', ha='center', fontsize=10, color='green')\n",
                "\n",
                "# Axon (output)\n",
                "ax.annotate('', xy=(0.85, 0.5), xytext=(0.5, 0.5),\n",
                "            arrowprops=dict(arrowstyle='->', color='red', lw=3))\n",
                "ax.text(0.67, 0.55, 'Axon (Output)', ha='center', fontsize=10, color='red')\n",
                "\n",
                "# Synapse\n",
                "ax.scatter([0.85], [0.5], s=100, c='orange', zorder=5)\n",
                "ax.text(0.85, 0.42, 'Synapse', ha='center', fontsize=9)\n",
                "\n",
                "# Threshold annotation\n",
                "ax.annotate('If signal > threshold:\\nNeuron \"fires\"', xy=(0.4, 0.38), \n",
                "            xytext=(0.4, 0.15), fontsize=9, ha='center',\n",
                "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
                "\n",
                "ax.set_xlim(-0.1, 1)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.set_aspect('equal')\n",
                "ax.axis('off')\n",
                "ax.set_title('Biological Neuron: The Inspiration', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 From Biology to Mathematics\n",
                "\n",
                "We abstract the biological neuron into a mathematical model:\n",
                "\n",
                "| Biological | Mathematical |\n",
                "|------------|-------------|\n",
                "| Dendrites receive signals | Inputs $x_1, x_2, ..., x_n$ |\n",
                "| Synaptic strength | Weights $w_1, w_2, ..., w_n$ |\n",
                "| Signal accumulation | Weighted sum $\\sum w_i x_i$ |\n",
                "| Firing threshold | Bias $b$ |\n",
                "| Fire or not | Activation function $f$ |\n",
                "| Axon output | Output $y$ |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: Mathematical Model of a Neuron\n",
                "\n",
                "---\n",
                "\n",
                "## 2.1 The Artificial Neuron\n",
                "\n",
                "An artificial neuron computes:\n",
                "\n",
                "$$z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
                "\n",
                "$$y = f(z)$$\n",
                "\n",
                "Where:\n",
                "- $\\mathbf{x} = [x_1, x_2, ..., x_n]$ are the **inputs**\n",
                "- $\\mathbf{w} = [w_1, w_2, ..., w_n]$ are the **weights**\n",
                "- $b$ is the **bias**\n",
                "- $z$ is the **pre-activation** (weighted sum)\n",
                "- $f$ is the **activation function**\n",
                "- $y$ is the **output**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Artificial Neuron\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Inputs\n",
                "input_y = [0.8, 0.5, 0.2]\n",
                "for i, y in enumerate(input_y):\n",
                "    ax.scatter([0.1], [y], s=200, c='lightgreen', edgecolors='black', zorder=5)\n",
                "    ax.text(0.02, y, f'$x_{i+1}$', ha='center', va='center', fontsize=12)\n",
                "\n",
                "# Weights on edges\n",
                "for i, y in enumerate(input_y):\n",
                "    ax.annotate('', xy=(0.4, 0.5), xytext=(0.15, y),\n",
                "                arrowprops=dict(arrowstyle='->', color='blue', lw=2))\n",
                "    mid_y = (y + 0.5) / 2\n",
                "    ax.text(0.25, mid_y + 0.05, f'$w_{i+1}$', fontsize=10, color='blue')\n",
                "\n",
                "# Neuron body\n",
                "neuron = plt.Circle((0.5, 0.5), 0.12, color='lightyellow', ec='black', linewidth=2)\n",
                "ax.add_patch(neuron)\n",
                "ax.text(0.5, 0.55, '$\\Sigma$', ha='center', va='center', fontsize=16)\n",
                "ax.text(0.5, 0.43, '$f$', ha='center', va='center', fontsize=14)\n",
                "\n",
                "# Bias\n",
                "ax.annotate('', xy=(0.5, 0.62), xytext=(0.5, 0.85),\n",
                "            arrowprops=dict(arrowstyle='->', color='purple', lw=2))\n",
                "ax.text(0.5, 0.9, '$b$ (bias)', ha='center', fontsize=10, color='purple')\n",
                "\n",
                "# Output\n",
                "ax.annotate('', xy=(0.85, 0.5), xytext=(0.62, 0.5),\n",
                "            arrowprops=dict(arrowstyle='->', color='red', lw=3))\n",
                "ax.scatter([0.9], [0.5], s=200, c='salmon', edgecolors='black', zorder=5)\n",
                "ax.text(0.9, 0.5, '$y$', ha='center', va='center', fontsize=12)\n",
                "\n",
                "# Equations\n",
                "ax.text(0.5, 0.1, '$z = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$\\n$y = f(z)$', \n",
                "        ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
                "\n",
                "ax.set_xlim(-0.1, 1)\n",
                "ax.set_ylim(0, 1)\n",
                "ax.set_aspect('equal')\n",
                "ax.axis('off')\n",
                "ax.set_title('Artificial Neuron: Mathematical Model', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Understanding Each Component\n",
                "\n",
                "### Weights ($w_i$)\n",
                "\n",
                "- Determine the **importance** of each input\n",
                "- Positive weight: input increases output\n",
                "- Negative weight: input decreases output\n",
                "- Zero weight: input is ignored\n",
                "\n",
                "### Bias ($b$)\n",
                "\n",
                "- Acts as a **threshold adjustment**\n",
                "- Allows the neuron to shift its activation\n",
                "- Can fire even when all inputs are zero (if $b$ is large enough)\n",
                "\n",
                "### Activation Function ($f$)\n",
                "\n",
                "- Introduces **non-linearity**\n",
                "- Without it, the network would just be a linear function\n",
                "- Common choices: sigmoid, ReLU, tanh (covered in Module 05)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Why Do We Need Bias?\n",
                "\n",
                "Without bias, the neuron's decision boundary must pass through the origin."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Effect of bias\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "x = np.linspace(-5, 5, 100)\n",
                "\n",
                "# Without bias: z = w*x, decision boundary at x=0\n",
                "ax = axes[0]\n",
                "w = 1\n",
                "z_no_bias = w * x\n",
                "y_no_bias = 1 / (1 + np.exp(-z_no_bias))  # sigmoid\n",
                "\n",
                "ax.plot(x, y_no_bias, 'b-', linewidth=2, label='sigmoid(w*x)')\n",
                "ax.axvline(x=0, color='red', linestyle='--', label='Decision boundary')\n",
                "ax.axhline(y=0.5, color='gray', linestyle=':')\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('y')\n",
                "ax.set_title('Without Bias: Boundary at x=0')\n",
                "ax.legend()\n",
                "ax.set_ylim(-0.1, 1.1)\n",
                "\n",
                "# With bias: z = w*x + b, decision boundary shifted\n",
                "ax = axes[1]\n",
                "b = 2\n",
                "z_with_bias = w * x + b\n",
                "y_with_bias = 1 / (1 + np.exp(-z_with_bias))\n",
                "\n",
                "ax.plot(x, y_with_bias, 'b-', linewidth=2, label='sigmoid(w*x + b)')\n",
                "ax.axvline(x=-b/w, color='red', linestyle='--', label=f'Decision boundary (x={-b/w})')\n",
                "ax.axhline(y=0.5, color='gray', linestyle=':')\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('y')\n",
                "ax.set_title(f'With Bias (b={b}): Boundary Shifted')\n",
                "ax.legend()\n",
                "ax.set_ylim(-0.1, 1.1)\n",
                "\n",
                "plt.suptitle('Bias Allows Shifting the Decision Boundary', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Geometric Interpretation\n",
                "\n",
                "For a 2D input, the neuron computes:\n",
                "$$z = w_1 x_1 + w_2 x_2 + b$$\n",
                "\n",
                "The decision boundary (where $z = 0$) is a **line**:\n",
                "$$w_1 x_1 + w_2 x_2 + b = 0$$\n",
                "\n",
                "- **Weights** determine the line's orientation\n",
                "- **Bias** shifts the line"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Decision boundary in 2D\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "x1 = np.linspace(-3, 3, 100)\n",
                "\n",
                "# Different weight combinations\n",
                "configs = [\n",
                "    ([1, 1], 0, 'w=[1,1], b=0'),\n",
                "    ([1, -1], 0, 'w=[1,-1], b=0'),\n",
                "    ([1, 1], 2, 'w=[1,1], b=2')\n",
                "]\n",
                "\n",
                "for ax, (w, b, title) in zip(axes, configs):\n",
                "    # Decision boundary: w1*x1 + w2*x2 + b = 0 => x2 = -(w1*x1 + b)/w2\n",
                "    x2 = -(w[0] * x1 + b) / w[1]\n",
                "    \n",
                "    ax.plot(x1, x2, 'r-', linewidth=2, label='Decision boundary')\n",
                "    \n",
                "    # Fill positive and negative regions\n",
                "    ax.fill_between(x1, x2, 3, alpha=0.3, color='blue', label='z > 0')\n",
                "    ax.fill_between(x1, x2, -3, alpha=0.3, color='orange', label='z < 0')\n",
                "    \n",
                "    ax.set_xlim(-3, 3)\n",
                "    ax.set_ylim(-3, 3)\n",
                "    ax.set_xlabel('$x_1$')\n",
                "    ax.set_ylabel('$x_2$')\n",
                "    ax.set_title(title)\n",
                "    ax.legend(loc='lower right', fontsize=8)\n",
                "    ax.set_aspect('equal')\n",
                "\n",
                "plt.suptitle('Neuron as a Linear Classifier in 2D', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 3: Implementing a Neuron from Scratch\n",
                "\n",
                "---\n",
                "\n",
                "## 3.1 NumPy Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    \"\"\"Sigmoid activation function.\"\"\"\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "class NeuronNumPy:\n",
                "    \"\"\"A single neuron implemented in NumPy.\"\"\"\n",
                "    \n",
                "    def __init__(self, n_inputs):\n",
                "        \"\"\"Initialize weights and bias randomly.\"\"\"\n",
                "        # Random initialization (small values)\n",
                "        self.weights = np.random.randn(n_inputs) * 0.01\n",
                "        self.bias = 0.0\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Compute the output for input x.\"\"\"\n",
                "        # Step 1: Weighted sum\n",
                "        z = np.dot(self.weights, x) + self.bias\n",
                "        \n",
                "        # Step 2: Activation\n",
                "        y = sigmoid(z)\n",
                "        \n",
                "        return y\n",
                "    \n",
                "    def __repr__(self):\n",
                "        return f\"NeuronNumPy(weights={self.weights}, bias={self.bias:.4f})\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the NumPy neuron\n",
                "neuron = NeuronNumPy(n_inputs=3)\n",
                "print(f\"Neuron: {neuron}\")\n",
                "\n",
                "# Single input\n",
                "x = np.array([1.0, 2.0, 3.0])\n",
                "output = neuron.forward(x)\n",
                "print(f\"\\nInput: {x}\")\n",
                "print(f\"Output: {output:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's trace through the calculation\n",
                "print(\"Step-by-step calculation:\")\n",
                "print(f\"Weights: {neuron.weights}\")\n",
                "print(f\"Bias: {neuron.bias}\")\n",
                "print(f\"Input: {x}\")\n",
                "\n",
                "z = np.dot(neuron.weights, x) + neuron.bias\n",
                "print(f\"\\nz = w.x + b = {z:.4f}\")\n",
                "\n",
                "y = sigmoid(z)\n",
                "print(f\"y = sigmoid(z) = {y:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 PyTorch Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NeuronPyTorch:\n",
                "    \"\"\"A single neuron implemented in PyTorch.\"\"\"\n",
                "    \n",
                "    def __init__(self, n_inputs):\n",
                "        \"\"\"Initialize weights and bias with gradient tracking.\"\"\"\n",
                "        self.weights = torch.randn(n_inputs, requires_grad=True) * 0.01\n",
                "        self.bias = torch.zeros(1, requires_grad=True)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Compute the output for input x.\"\"\"\n",
                "        z = torch.dot(self.weights, x) + self.bias\n",
                "        y = torch.sigmoid(z)\n",
                "        return y\n",
                "    \n",
                "    def __repr__(self):\n",
                "        return f\"NeuronPyTorch(weights={self.weights.data}, bias={self.bias.item():.4f})\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the PyTorch neuron\n",
                "neuron_pt = NeuronPyTorch(n_inputs=3)\n",
                "print(f\"Neuron: {neuron_pt}\")\n",
                "\n",
                "x_pt = torch.tensor([1.0, 2.0, 3.0])\n",
                "output_pt = neuron_pt.forward(x_pt)\n",
                "print(f\"\\nInput: {x_pt}\")\n",
                "print(f\"Output: {output_pt.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Using PyTorch's nn.Linear\n",
                "\n",
                "PyTorch provides `nn.Linear` which implements a linear layer (without activation). A single neuron is just a linear layer with one output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "\n",
                "# A single neuron using nn.Linear\n",
                "class NeuronModule(nn.Module):\n",
                "    def __init__(self, n_inputs):\n",
                "        super().__init__()\n",
                "        # Linear layer: n_inputs -> 1 output\n",
                "        self.linear = nn.Linear(n_inputs, 1)\n",
                "        self.activation = nn.Sigmoid()\n",
                "    \n",
                "    def forward(self, x):\n",
                "        z = self.linear(x)\n",
                "        y = self.activation(z)\n",
                "        return y"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the nn.Module neuron\n",
                "neuron_module = NeuronModule(n_inputs=3)\n",
                "\n",
                "print(\"Neuron parameters:\")\n",
                "for name, param in neuron_module.named_parameters():\n",
                "    print(f\"  {name}: {param.data}\")\n",
                "\n",
                "x_pt = torch.tensor([1.0, 2.0, 3.0])\n",
                "output = neuron_module(x_pt)\n",
                "print(f\"\\nInput: {x_pt}\")\n",
                "print(f\"Output: {output.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.4 Processing Multiple Inputs (Batch Processing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Batch of inputs\n",
                "batch_size = 4\n",
                "n_features = 3\n",
                "\n",
                "X = torch.randn(batch_size, n_features)\n",
                "print(f\"Batch input shape: {X.shape}\")\n",
                "print(f\"Input batch:\\n{X}\")\n",
                "\n",
                "# Process entire batch at once\n",
                "outputs = neuron_module(X)\n",
                "print(f\"\\nOutput shape: {outputs.shape}\")\n",
                "print(f\"Outputs: {outputs.squeeze()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4: Learning in a Neuron\n",
                "\n",
                "---\n",
                "\n",
                "## 4.1 How Does a Neuron Learn?\n",
                "\n",
                "Learning means finding the best weights and bias. We:\n",
                "1. Make a prediction\n",
                "2. Compute the error (loss)\n",
                "3. Compute gradients (how to adjust weights)\n",
                "4. Update weights in the direction that reduces error"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple training example\n",
                "# Task: Learn to output 1 for [1, 0] and 0 for [0, 1]\n",
                "\n",
                "# Training data\n",
                "X_train = torch.tensor([[1.0, 0.0],\n",
                "                        [0.0, 1.0]])\n",
                "y_train = torch.tensor([[1.0], [0.0]])\n",
                "\n",
                "# Create neuron\n",
                "neuron = NeuronModule(n_inputs=2)\n",
                "\n",
                "# Loss function and optimizer\n",
                "criterion = nn.MSELoss()\n",
                "optimizer = torch.optim.SGD(neuron.parameters(), lr=1.0)\n",
                "\n",
                "# Before training\n",
                "print(\"Before training:\")\n",
                "with torch.no_grad():\n",
                "    predictions = neuron(X_train)\n",
                "    print(f\"Predictions: {predictions.squeeze().tolist()}\")\n",
                "    print(f\"Targets: {y_train.squeeze().tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "losses = []\n",
                "n_epochs = 100\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    # Forward pass\n",
                "    predictions = neuron(X_train)\n",
                "    loss = criterion(predictions, y_train)\n",
                "    \n",
                "    # Backward pass\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    losses.append(loss.item())\n",
                "    \n",
                "    if epoch % 20 == 0:\n",
                "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
                "\n",
                "# After training\n",
                "print(\"\\nAfter training:\")\n",
                "with torch.no_grad():\n",
                "    predictions = neuron(X_train)\n",
                "    print(f\"Predictions: {predictions.squeeze().tolist()}\")\n",
                "    print(f\"Targets: {y_train.squeeze().tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training loss\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(losses)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss Over Time')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 What Did the Neuron Learn?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Examine learned parameters\n",
                "print(\"Learned parameters:\")\n",
                "for name, param in neuron.named_parameters():\n",
                "    print(f\"  {name}: {param.data}\")\n",
                "\n",
                "# The neuron learned:\n",
                "# - Positive weight for x1 (first input important for output=1)\n",
                "# - Negative weight for x2 (second input important for output=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 5: Limitations of a Single Neuron\n",
                "\n",
                "---\n",
                "\n",
                "## 5.1 A Single Neuron is a Linear Classifier\n",
                "\n",
                "A single neuron can only learn **linearly separable** patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: AND gate (linearly separable - a single neuron can learn this)\n",
                "X_and = torch.tensor([[0.0, 0.0],\n",
                "                      [0.0, 1.0],\n",
                "                      [1.0, 0.0],\n",
                "                      [1.0, 1.0]])\n",
                "y_and = torch.tensor([[0.0], [0.0], [0.0], [1.0]])\n",
                "\n",
                "# Train neuron on AND\n",
                "neuron_and = NeuronModule(2)\n",
                "optimizer = torch.optim.SGD(neuron_and.parameters(), lr=5.0)\n",
                "\n",
                "for _ in range(500):\n",
                "    pred = neuron_and(X_and)\n",
                "    loss = nn.MSELoss()(pred, y_and)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "print(\"AND Gate:\")\n",
                "print(\"Input -> Prediction (Target)\")\n",
                "with torch.no_grad():\n",
                "    for x, y in zip(X_and, y_and):\n",
                "        pred = neuron_and(x)\n",
                "        print(f\"{x.tolist()} -> {pred.item():.2f} ({y.item()})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: XOR gate (NOT linearly separable - single neuron fails!)\n",
                "X_xor = torch.tensor([[0.0, 0.0],\n",
                "                      [0.0, 1.0],\n",
                "                      [1.0, 0.0],\n",
                "                      [1.0, 1.0]])\n",
                "y_xor = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
                "\n",
                "# Train neuron on XOR\n",
                "neuron_xor = NeuronModule(2)\n",
                "optimizer = torch.optim.SGD(neuron_xor.parameters(), lr=5.0)\n",
                "\n",
                "for _ in range(500):\n",
                "    pred = neuron_xor(X_xor)\n",
                "    loss = nn.MSELoss()(pred, y_xor)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "print(\"XOR Gate (single neuron FAILS):\")\n",
                "print(\"Input -> Prediction (Target)\")\n",
                "with torch.no_grad():\n",
                "    for x, y in zip(X_xor, y_xor):\n",
                "        pred = neuron_xor(x)\n",
                "        print(f\"{x.tolist()} -> {pred.item():.2f} ({y.item()})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Why XOR fails\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# AND gate\n",
                "ax = axes[0]\n",
                "colors = ['red' if y == 0 else 'blue' for y in y_and]\n",
                "ax.scatter(X_and[:, 0], X_and[:, 1], c=colors, s=200, edgecolors='black')\n",
                "\n",
                "# Decision boundary for AND\n",
                "w = neuron_and.linear.weight.data.squeeze().numpy()\n",
                "b = neuron_and.linear.bias.data.item()\n",
                "x_line = np.linspace(-0.5, 1.5, 100)\n",
                "y_line = -(w[0] * x_line + b) / w[1]\n",
                "ax.plot(x_line, y_line, 'g--', linewidth=2, label='Decision boundary')\n",
                "\n",
                "ax.set_xlim(-0.5, 1.5)\n",
                "ax.set_ylim(-0.5, 1.5)\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('AND Gate: Linearly Separable')\n",
                "ax.legend()\n",
                "\n",
                "# XOR gate\n",
                "ax = axes[1]\n",
                "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
                "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black')\n",
                "\n",
                "ax.set_xlim(-0.5, 1.5)\n",
                "ax.set_ylim(-0.5, 1.5)\n",
                "ax.set_xlabel('$x_1$')\n",
                "ax.set_ylabel('$x_2$')\n",
                "ax.set_title('XOR Gate: NOT Linearly Separable\\n(No single line can separate red and blue)')\n",
                "\n",
                "ax.annotate('No single line\\ncan separate!', xy=(0.5, 0.5), fontsize=10, ha='center',\n",
                "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 The Solution: Multiple Neurons (Neural Networks)\n",
                "\n",
                "By combining multiple neurons in layers, we can learn non-linear patterns. This is the foundation of **deep learning**.\n",
                "\n",
                "We'll explore this in detail in Module 06 (Perceptron) and beyond."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Key Points Summary\n",
                "\n",
                "---\n",
                "\n",
                "## Biological Inspiration\n",
                "- Artificial neurons are inspired by biological neurons\n",
                "- Dendrites = inputs, synapses = weights, firing = activation\n",
                "\n",
                "## Mathematical Model\n",
                "- $z = \\mathbf{w}^T \\mathbf{x} + b$ (weighted sum + bias)\n",
                "- $y = f(z)$ (apply activation function)\n",
                "- Weights determine importance of inputs\n",
                "- Bias shifts the decision boundary\n",
                "\n",
                "## Implementation\n",
                "- NumPy: Manual implementation for understanding\n",
                "- PyTorch: Use nn.Linear + activation\n",
                "- Batch processing: Handles multiple inputs efficiently\n",
                "\n",
                "## Limitations\n",
                "- Single neuron = linear classifier\n",
                "- Cannot learn non-linear patterns (like XOR)\n",
                "- Solution: Combine neurons into networks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Interview Tips\n",
                "\n",
                "---\n",
                "\n",
                "## Common Questions\n",
                "\n",
                "**Q: What is an artificial neuron?**\n",
                "A: An artificial neuron computes a weighted sum of inputs plus a bias, then applies an activation function. Mathematically: y = f(w.x + b). It's the basic building block of neural networks.\n",
                "\n",
                "**Q: Why do we need bias in a neuron?**\n",
                "A: Bias allows the decision boundary to shift away from the origin. Without bias, the neuron's output would be zero when all inputs are zero, limiting what it can learn.\n",
                "\n",
                "**Q: Why do we need activation functions?**\n",
                "A: Without activation functions, a neural network would just compute a linear function regardless of depth. Activation functions introduce non-linearity, allowing networks to learn complex patterns.\n",
                "\n",
                "**Q: Can a single neuron learn any function?**\n",
                "A: No. A single neuron is a linear classifier and can only learn linearly separable patterns. For example, it cannot learn XOR. This is why we need multiple neurons organized in layers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Practice Exercises\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Manual Calculation\n",
                "\n",
                "Given: w = [0.5, -0.3, 0.8], b = 0.1, x = [1.0, 2.0, 0.5]\n",
                "\n",
                "Calculate z and y (using sigmoid) by hand, then verify with code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your calculation:\n",
                "# z = 0.5*1.0 + (-0.3)*2.0 + 0.8*0.5 + 0.1 = ?\n",
                "# y = sigmoid(z) = ?\n",
                "\n",
                "# Verify with code\n",
                "w = np.array([0.5, -0.3, 0.8])\n",
                "b = 0.1\n",
                "x = np.array([1.0, 2.0, 0.5])\n",
                "\n",
                "# Your code here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 2: Implement OR Gate\n",
                "\n",
                "Train a single neuron to learn the OR gate:\n",
                "- (0,0) -> 0\n",
                "- (0,1) -> 1\n",
                "- (1,0) -> 1\n",
                "- (1,1) -> 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "X_or = torch.tensor([[0.0, 0.0],\n",
                "                     [0.0, 1.0],\n",
                "                     [1.0, 0.0],\n",
                "                     [1.0, 1.0]])\n",
                "y_or = torch.tensor([[0.0], [1.0], [1.0], [1.0]])\n",
                "\n",
                "# Create and train neuron\n",
                "# ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1 Solution\n",
                "print(\"Exercise 1:\")\n",
                "w = np.array([0.5, -0.3, 0.8])\n",
                "b = 0.1  \n",
                "x = np.array([1.0, 2.0, 0.5])\n",
                "\n",
                "z = np.dot(w, x) + b\n",
                "y = 1 / (1 + np.exp(-z))\n",
                "\n",
                "print(f\"z = 0.5*1.0 + (-0.3)*2.0 + 0.8*0.5 + 0.1\")\n",
                "print(f\"z = 0.5 - 0.6 + 0.4 + 0.1 = {z}\")\n",
                "print(f\"y = sigmoid({z}) = {y:.4f}\")\n",
                "\n",
                "# Exercise 2 Solution\n",
                "print(\"\\nExercise 2:\")\n",
                "neuron_or = NeuronModule(2)\n",
                "optimizer = torch.optim.SGD(neuron_or.parameters(), lr=5.0)\n",
                "\n",
                "for _ in range(500):\n",
                "    pred = neuron_or(X_or)\n",
                "    loss = nn.MSELoss()(pred, y_or)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "print(\"OR Gate:\")\n",
                "with torch.no_grad():\n",
                "    for x, y in zip(X_or, y_or):\n",
                "        pred = neuron_or(x)\n",
                "        print(f\"{x.tolist()} -> {pred.item():.2f} ({y.item()})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Module: [05 - Activation Functions](../05_activation_functions/05_activation_functions.ipynb)\n",
                "\n",
                "Now that we understand the neuron, let's dive deep into activation functions - the key to non-linearity in neural networks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}